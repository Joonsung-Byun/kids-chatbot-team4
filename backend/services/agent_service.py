"""
LangChain Agent Service (LangChain 1.0+ with LangGraph)

LangGraphì˜ create_react_agentë¥¼ ì‚¬ìš©í•œ ë„êµ¬ ìë™ ì„ íƒ ë° ì‹¤í–‰
"""

import json
from typing import List, Dict, Any

from langchain_core.tools import tool
from langchain_core.messages import HumanMessage, AIMessage

from services.llm_service import get_llm_service
from services.rag_service import get_rag_service
from services.weather_service import get_weather
from services.map_service import get_map_markers
from utils.logger import logger


# ============================================================
# Tool ì •ì˜ (LangGraph ë°©ì‹)
# ============================================================

@tool
def weather_tool(location: str) -> str:
    """ë‚ ì”¨ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤. ì…ë ¥: ì§€ì—­ëª… (ì˜ˆ: 'ì„œìš¸', 'ê°•ë‚¨')"""
    try:
        logger.info(f"[WeatherTool] í˜¸ì¶œ: {location}")
        result = get_weather(location=location, target_date=None)
        return json.dumps(result, ensure_ascii=False)
    except Exception as e:
        logger.error(f"[WeatherTool] ì˜¤ë¥˜: {e}")
        return json.dumps({"error": str(e)})


@tool
def rag_search_tool(query: str) -> str:
    """ë¬¸í™”/ì²´ìœ¡/êµìœ¡ ì‹œì„¤ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤. ì…ë ¥: ê²€ìƒ‰ ì¿¼ë¦¬ (ì˜ˆ: 'ì„œìš¸ ë†€ì´í„°', 'ê°•ë‚¨ í‚¤ì¦ˆì¹´í˜')"""
    try:
        logger.info(f"[RAGTool] í˜¸ì¶œ: {query}")
        rag_service = get_rag_service()
        results = rag_service.search_and_rerank(query=query, top_k=5)
        
        formatted = []
        for doc in results[:3]:
            metadata = doc.get("metadata", {})
            formatted.append({
                "name": metadata.get("facility_name", "Unknown"),
                "category": metadata.get("category1", "ì‹œì„¤"),
                "location": metadata.get("region_gu", ""),
                "price": metadata.get("price", "ë¬´ë£Œ")
            })
        return json.dumps(formatted, ensure_ascii=False)
    except Exception as e:
        logger.error(f"[RAGTool] ì˜¤ë¥˜: {e}")
        return json.dumps({"error": str(e)})


@tool
def map_tool(query: str) -> str:
    """ì§€ë„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì…ë ¥: ì‹œì„¤ ì •ë³´"""
    try:
        logger.info(f"[MapTool] í˜¸ì¶œ: {query}")
        # TODO: ì‹¤ì œ ì¹´ì¹´ì˜¤ë§µ API ì—°ë™
        return json.dumps({
            "status": "success",
            "message": "ì§€ë„ ìƒì„± ì™„ë£Œ (Mock)"
        }, ensure_ascii=False)
    except Exception as e:
        logger.error(f"[MapTool] ì˜¤ë¥˜: {e}")
        return json.dumps({"error": str(e)})


# ============================================================
# Tools ë¦¬ìŠ¤íŠ¸
# ============================================================

def get_tools():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡ ë°˜í™˜"""
    return [weather_tool, rag_search_tool, map_tool]


# ============================================================
# Mock LLM (CPU í™˜ê²½ìš©)
# ============================================================

class MockChatModel:
    """CPU í™˜ê²½ì—ì„œ ì‚¬ìš©í•  ê°„ë‹¨í•œ Mock ChatModel"""
    
    def __init__(self):
        self.model = "mock-chat-model"
    
    def invoke(self, messages):
        """ê°„ë‹¨í•œ ë£°ë² ì´ìŠ¤ ì‘ë‹µ"""
        if isinstance(messages, list) and messages:
            last_msg = messages[-1]
            user_input = getattr(last_msg, "content", str(last_msg))
        else:
            user_input = str(messages)
        
        if any(k in user_input for k in ["ì§€ì—­", "ì–´ë””", "ìœ„ì¹˜"]):
            response = "ì–´ëŠ ì§€ì—­ì„ ìƒê°í•˜ê³  ê³„ì‹ ê°€ìš”? ğŸ—ºï¸ (ì„œìš¸, ë¶€ì‚°, ëŒ€êµ¬ ë“±)"
        elif any(k in user_input for k in ["ê³ ë§ˆì›Œ", "ê°ì‚¬", "ì¢‹ì•„"]):
            response = "ì²œë§Œì—ìš”! ğŸ˜Š ë” ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ì£¼ì„¸ìš”!"
        else:
            response = "Mock ëª¨ë“œì…ë‹ˆë‹¤. ì‹¤ì œ LLMì„ ì‚¬ìš©í•˜ë ¤ë©´ OpenAI API í‚¤ë¥¼ ì„¤ì •í•˜ê±°ë‚˜ GPU í™˜ê²½ì—ì„œ ì‹¤í–‰í•´ì£¼ì„¸ìš”."
        
        return AIMessage(content=response)
    
    def bind_tools(self, tools):
        return self


# ============================================================
# Agent ìƒì„±
# ============================================================

def create_langchain_agent():
    """LangGraph create_react_agentë¥¼ ì‚¬ìš©í•œ Agent ìƒì„±"""
    logger.info("ğŸ”§ LangGraph Agent ìƒì„± ì¤‘...")
    
    try:
        from langgraph.prebuilt import create_react_agent
    except ImportError:
        logger.error("âŒ langgraph íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install langgraph ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.")
        return None
    
    tools = get_tools()
    
    # âœ… í˜„ì¬ ë²„ì „(1.0.2)ì€ system_message ì¸ì ì‚¬ìš©
    system_prompt = """ë‹¹ì‹ ì€ ì•„ì´ì™€ í•¨ê»˜í•  ìˆ˜ ìˆëŠ” í™œë™ì„ ì¶”ì²œí•˜ëŠ” ì „ë¬¸ ì±—ë´‡ì…ë‹ˆë‹¤.

**ì¤‘ìš” ê·œì¹™:**

1. **ìœ„ì¹˜ í™•ì¸ì´ ìµœìš°ì„ ì…ë‹ˆë‹¤:**
   - ì‚¬ìš©ì ì§ˆë¬¸ì— ì§€ì—­ëª…ì´ ì—†ìœ¼ë©´ ë¨¼ì € "ì–´ëŠ ì§€ì—­ì„ ìƒê°í•˜ê³  ê³„ì‹ ê°€ìš”? ğŸ—ºï¸" ì§ˆë¬¸
   - ì´ì „ ëŒ€í™”ì— ì§€ì—­ì´ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ì‚¬ìš©

2. **ìœ„ì¹˜ê°€ í™•ì¸ë˜ë©´ ë‹¤ìŒ ìˆœì„œë¡œ ì§„í–‰:**
   - Step 1: weather_toolë¡œ ë‚ ì”¨ í™•ì¸
   - Step 2: rag_search_toolë¡œ ì¶”ì²œ ì‹œì„¤ ê²€ìƒ‰
   - Step 3: ê²°ê³¼ë¥¼ ì¢…í•©í•´ì„œ ì¹œì ˆí•˜ê²Œ ë‹µë³€

3. **ê°ì • í‘œí˜„("ê³ ë§ˆì›Œ", "ì¢‹ì•„ìš”" ë“±):**
   - ë„êµ¬ ì‚¬ìš© ì—†ì´ ë°”ë¡œ ì¹œì ˆí•˜ê²Œ ì‘ë‹µ

4. **ë‹µë³€ ìŠ¤íƒ€ì¼:**
   - ì´ëª¨ì§€ ì‚¬ìš© (ğŸ¨, ğŸƒâ€â™‚ï¸, ğŸ“)
   - êµ¬ì²´ì ì¸ ì •ë³´ ì œê³µ
   - ì¶”ê°€ ì§ˆë¬¸ ìœ ë„

ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ë¥¼ í™œìš©í•˜ì—¬ ìµœì„ ì˜ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”."""
    
    # LLM ë¡œë“œ
    llm_service = get_llm_service()
    
    if llm_service._use_gpu and llm_service._model:
        try:
            from langchain_huggingface import HuggingFacePipeline
            llm = HuggingFacePipeline(
                pipeline=llm_service._model,
                model_kwargs={"temperature": 0.7, "max_new_tokens": 512}
            )
            logger.info("âœ… GPU ëª¨ë“œ: HuggingFace LLM ì‚¬ìš©")
        except ImportError:
            logger.warning("âš ï¸ langchain-huggingface ë¯¸ì„¤ì¹˜ â†’ Mock ëª¨ë“œ ì „í™˜")
            llm = MockChatModel()
    else:
        logger.info("âœ… CPU ëª¨ë“œ: Mock LLM ì‚¬ìš©")
        llm = MockChatModel()
    
    try:
        agent = create_react_agent(
            model=llm,
            tools=tools,
            system_prompt=system_prompt
        )
        logger.info("âœ… LangGraph ReAct Agent ìƒì„± ì™„ë£Œ")
        return agent
    except Exception as e:
        logger.error(f"âŒ Agent ìƒì„± ì‹¤íŒ¨: {e}")
        return None


# ============================================================
# ëŒ€í™” íˆìŠ¤í† ë¦¬ ë³€í™˜
# ============================================================

def convert_history_to_messages(history: List[Dict[str, str]]) -> List:
    """ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ LangChain Message í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    messages = []
    for msg in history[-5:]:
        if msg["role"] == "user":
            messages.append(HumanMessage(content=msg["content"]))
        elif msg["role"] == "ai":
            messages.append(AIMessage(content=msg["content"]))
    return messages


# ============================================================
# Agent ì‹¤í–‰
# ============================================================

def run_agent(user_query: str, conversation_id: str, conversation_history: List[Dict[str, str]] = None) -> Dict[str, Any]:
    """LangGraph Agent ì‹¤í–‰"""
    logger.info(f"ğŸš€ Agent ì‹¤í–‰: conversation_id={conversation_id}")
    
    agent = create_langchain_agent()
    
    if agent is None:
        return {
            "answer": "ì£„ì†¡í•©ë‹ˆë‹¤. Agent ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. langgraph íŒ¨í‚¤ì§€ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.",
            "conversation_history": conversation_history or [],
            "tools_used": []
        }
    
    chat_history = convert_history_to_messages(conversation_history or [])
    all_messages = chat_history + [HumanMessage(content=user_query)]
    
    try:
        result = agent.invoke({"messages": all_messages})
        answer = ""
        tools_used = []
        
        if "messages" in result:
            for msg in reversed(result["messages"]):
                if isinstance(msg, AIMessage):
                    answer = msg.content
                    break
            for msg in result["messages"]:
                if isinstance(msg, AIMessage) and getattr(msg, "tool_calls", None):
                    tools_used.extend(t["name"] for t in msg.tool_calls if isinstance(t, dict) and "name" in t)
        
        if not answer:
            answer = "ì‘ë‹µ ìƒì„± ì‹¤íŒ¨"
        
        new_history = (conversation_history or []) + [
            {"role": "user", "content": user_query},
            {"role": "ai", "content": answer},
        ]
        
        logger.info(f"âœ… Agent ì™„ë£Œ (ì‚¬ìš©ëœ ë„êµ¬: {list(set(tools_used))})")
        return {"answer": answer, "conversation_history": new_history, "tools_used": list(set(tools_used))}
    
    except Exception as e:
        logger.error(f"âŒ Agent ì‹¤í–‰ ì˜¤ë¥˜: {e}", exc_info=True)
        fallback_answer = "ì£„ì†¡í•©ë‹ˆë‹¤. ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”. ğŸ™"
        new_history = (conversation_history or []) + [
            {"role": "user", "content": user_query},
            {"role": "ai", "content": fallback_answer},
        ]
        return {"answer": fallback_answer, "conversation_history": new_history, "tools_used": []}
