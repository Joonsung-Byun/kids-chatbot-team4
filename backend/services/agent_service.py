"""
LangChain Agent Service (LangChain 1.0+ with LangGraph)

LangGraphì˜ create_react_agentë¥¼ ì‚¬ìš©í•œ ë„êµ¬ ìë™ ì„ íƒ ë° ì‹¤í–‰
langgraph 1.0.2 ë²„ì „ í˜¸í™˜ + MockChatModel Runnable êµ¬í˜„
"""

import json
from typing import List, Dict, Any, Iterator

from langchain_core.tools import tool
from langchain_core.messages import HumanMessage, AIMessage, BaseMessage
from langchain_core.runnables import Runnable
from langchain_core.language_models import BaseChatModel
from langchain_core.outputs import ChatResult, ChatGeneration

from services.llm_service import get_llm_service
from services.rag_service import get_rag_service
from services.weather_service import get_weather
from services.map_service import get_map_markers
from utils.logger import logger


# ============================================================
# Tool ì •ì˜ (LangGraph ë°©ì‹)
# ============================================================

@tool
def weather_tool(location: str) -> str:
    """ë‚ ì”¨ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤. ì…ë ¥: ì§€ì—­ëª… (ì˜ˆ: 'ì„œìš¸', 'ê°•ë‚¨')"""
    try:
        logger.info(f"[WeatherTool] í˜¸ì¶œ: {location}")
        result = get_weather(location=location, target_date=None)
        return json.dumps(result, ensure_ascii=False)
    except Exception as e:
        logger.error(f"[WeatherTool] ì˜¤ë¥˜: {e}")
        return json.dumps({"error": str(e)})


@tool
def rag_search_tool(query: str) -> str:
    """ë¬¸í™”/ì²´ìœ¡/êµìœ¡ ì‹œì„¤ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤. ì…ë ¥: ê²€ìƒ‰ ì¿¼ë¦¬ (ì˜ˆ: 'ì„œìš¸ ë†€ì´í„°', 'ê°•ë‚¨ í‚¤ì¦ˆì¹´í˜')"""
    try:
        logger.info(f"[RAGTool] í˜¸ì¶œ: {query}")
        rag_service = get_rag_service()
        results = rag_service.search_and_rerank(query=query, top_k=5)
        
        formatted = []
        for doc in results[:3]:
            metadata = doc.get("metadata", {})
            formatted.append({
                "name": metadata.get("facility_name", "Unknown"),
                "category": metadata.get("category1", "ì‹œì„¤"),
                "location": metadata.get("region_gu", ""),
                "price": metadata.get("price", "ë¬´ë£Œ")
            })
        return json.dumps(formatted, ensure_ascii=False)
    except Exception as e:
        logger.error(f"[RAGTool] ì˜¤ë¥˜: {e}")
        return json.dumps({"error": str(e)})


@tool
def map_tool(query: str) -> str:
    """ì§€ë„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì…ë ¥: ì‹œì„¤ ì •ë³´"""
    try:
        logger.info(f"[MapTool] í˜¸ì¶œ: {query}")
        # TODO: ì‹¤ì œ ì¹´ì¹´ì˜¤ë§µ API ì—°ë™
        return json.dumps({
            "status": "success",
            "message": "ì§€ë„ ìƒì„± ì™„ë£Œ (Mock)"
        }, ensure_ascii=False)
    except Exception as e:
        logger.error(f"[MapTool] ì˜¤ë¥˜: {e}")
        return json.dumps({"error": str(e)})


# ============================================================
# Tools ë¦¬ìŠ¤íŠ¸
# ============================================================

def get_tools():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡ ë°˜í™˜"""
    return [weather_tool, rag_search_tool, map_tool]


# ============================================================
# Mock LLM (CPU í™˜ê²½ìš©) - Runnable êµ¬í˜„
# ============================================================

class MockChatModel(BaseChatModel):
    """
    LangGraph í˜¸í™˜ Mock ChatModel
    BaseChatModelì„ ìƒì†í•˜ì—¬ Runnable ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„
    """
    
    model_name: str = "mock-chat-model"
    
    def _generate(self, messages: List[BaseMessage], stop=None, **kwargs) -> ChatResult:
        """í•„ìˆ˜ ë©”ì„œë“œ: ë©”ì‹œì§€ ìƒì„±"""
        # ë§ˆì§€ë§‰ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ì¶œ
        user_input = ""
        for msg in reversed(messages):
            if isinstance(msg, HumanMessage):
                user_input = msg.content
                break
        
        # ë£° ë² ì´ìŠ¤ ì‘ë‹µ
        if any(k in user_input for k in ["ì§€ì—­", "ì–´ë””", "ìœ„ì¹˜"]):
            response_text = "ì–´ëŠ ì§€ì—­ì„ ìƒê°í•˜ê³  ê³„ì‹ ê°€ìš”? ğŸ—ºï¸ (ì„œìš¸, ë¶€ì‚°, ëŒ€êµ¬ ë“±)"
        elif any(k in user_input for k in ["ê³ ë§ˆì›Œ", "ê°ì‚¬", "ì¢‹ì•„"]):
            response_text = "ì²œë§Œì—ìš”! ğŸ˜Š ë” ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ì£¼ì„¸ìš”!"
        else:
            response_text = "Mock ëª¨ë“œì…ë‹ˆë‹¤. ì‹¤ì œ LLMì„ ì‚¬ìš©í•˜ë ¤ë©´ OpenAI API í‚¤ë¥¼ ì„¤ì •í•˜ê±°ë‚˜ GPU í™˜ê²½ì—ì„œ ì‹¤í–‰í•´ì£¼ì„¸ìš”."
        
        # ChatResult ë°˜í™˜
        message = AIMessage(content=response_text)
        generation = ChatGeneration(message=message)
        return ChatResult(generations=[generation])
    
    @property
    def _llm_type(self) -> str:
        """í•„ìˆ˜ ì†ì„±: LLM íƒ€ì…"""
        return "mock-chat-model"
    
    def bind_tools(self, tools, **kwargs):
        """ë„êµ¬ ë°”ì¸ë”© (Mockì—ì„œëŠ” self ë°˜í™˜)"""
        return self
    
    def _stream(self, messages: List[BaseMessage], stop=None, **kwargs) -> Iterator[ChatResult]:
        """ì„ íƒ ë©”ì„œë“œ: ìŠ¤íŠ¸ë¦¬ë° (ë¯¸êµ¬í˜„)"""
        result = self._generate(messages, stop=stop, **kwargs)
        yield result
    
    async def _agenerate(self, messages: List[BaseMessage], stop=None, **kwargs) -> ChatResult:
        """ì„ íƒ ë©”ì„œë“œ: ë¹„ë™ê¸° ìƒì„± (ë™ê¸° ë²„ì „ ì¬ì‚¬ìš©)"""
        return self._generate(messages, stop=stop, **kwargs)


# ============================================================
# Agent ìƒì„±
# ============================================================

def create_langchain_agent():
    """LangGraph create_react_agentë¥¼ ì‚¬ìš©í•œ Agent ìƒì„±"""
    logger.info("ğŸ”§ LangGraph Agent ìƒì„± ì¤‘...")
    
    try:
        from langgraph.prebuilt import create_react_agent
    except ImportError:
        logger.error("âŒ langgraph íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install langgraph ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.")
        return None
    
    tools = get_tools()
    
    # System prompt ì •ì˜
    system_prompt = """ë‹¹ì‹ ì€ ì•„ì´ì™€ í•¨ê»˜í•  ìˆ˜ ìˆëŠ” í™œë™ì„ ì¶”ì²œí•˜ëŠ” ì „ë¬¸ ì±—ë´‡ì…ë‹ˆë‹¤.

**ì¤‘ìš” ê·œì¹™:**

1. **ìœ„ì¹˜ í™•ì¸ì´ ìµœìš°ì„ ì…ë‹ˆë‹¤:**
   - ì‚¬ìš©ì ì§ˆë¬¸ì— ì§€ì—­ëª…ì´ ì—†ìœ¼ë©´ ë¨¼ì € "ì–´ëŠ ì§€ì—­ì„ ìƒê°í•˜ê³  ê³„ì‹ ê°€ìš”? ğŸ—ºï¸" ì§ˆë¬¸
   - ì´ì „ ëŒ€í™”ì— ì§€ì—­ì´ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ì‚¬ìš©

2. **ìœ„ì¹˜ê°€ í™•ì¸ë˜ë©´ ë‹¤ìŒ ìˆœì„œë¡œ ì§„í–‰:**
   - Step 1: weather_toolë¡œ ë‚ ì”¨ í™•ì¸
   - Step 2: rag_search_toolë¡œ ì¶”ì²œ ì‹œì„¤ ê²€ìƒ‰
   - Step 3: ê²°ê³¼ë¥¼ ì¢…í•©í•´ì„œ ì¹œì ˆí•˜ê²Œ ë‹µë³€

3. **ê°ì • í‘œí˜„("ê³ ë§ˆì›Œ", "ì¢‹ì•„ìš”" ë“±):**
   - ë„êµ¬ ì‚¬ìš© ì—†ì´ ë°”ë¡œ ì¹œì ˆí•˜ê²Œ ì‘ë‹µ

4. **ë‹µë³€ ìŠ¤íƒ€ì¼:**
   - ì´ëª¨ì§€ ì‚¬ìš© (ğŸ¨, ğŸƒâ€â™‚ï¸, ğŸ“)
   - êµ¬ì²´ì ì¸ ì •ë³´ ì œê³µ
   - ì¶”ê°€ ì§ˆë¬¸ ìœ ë„

ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ë¥¼ í™œìš©í•˜ì—¬ ìµœì„ ì˜ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”."""
    
    # LLM ë¡œë“œ
    llm_service = get_llm_service()
    
    # OpenAI API í‚¤ê°€ ìˆìœ¼ë©´ OpenAI ì‚¬ìš©
    import os
    if os.getenv("OPENAI_API_KEY"):
        try:
            from langchain_openai import ChatOpenAI
            llm = ChatOpenAI(model="gpt-4o", temperature=0.7)
            logger.info("âœ… OpenAI LLM ì‚¬ìš©")
        except ImportError:
            logger.warning("âš ï¸ langchain-openai ë¯¸ì„¤ì¹˜ â†’ Mock ëª¨ë“œ")
            llm = MockChatModel()
    elif llm_service._use_gpu and llm_service._model:
        try:
            from langchain_huggingface import HuggingFacePipeline
            llm = HuggingFacePipeline(
                pipeline=llm_service._model,
                model_kwargs={"temperature": 0.7, "max_new_tokens": 512}
            )
            logger.info("âœ… GPU ëª¨ë“œ: HuggingFace LLM ì‚¬ìš©")
        except ImportError:
            logger.warning("âš ï¸ langchain-huggingface ë¯¸ì„¤ì¹˜ â†’ Mock ëª¨ë“œ")
            llm = MockChatModel()
    else:
        logger.info("âœ… CPU ëª¨ë“œ: Mock LLM ì‚¬ìš©")
        llm = MockChatModel()
    
    # ============================================================
    # langgraph ë²„ì „ë³„ í˜¸í™˜ì„± ì²˜ë¦¬
    # ============================================================
    
    # langgraph 1.0.2ëŠ” íŒŒë¼ë¯¸í„°ê°€ ê±°ì˜ ì—†ìŒ!
    # ê³µì‹ ë¬¸ì„œ: create_react_agent(model, tools, checkpointer=None)
    
    try:
        logger.info("ğŸ”§ Agent ìƒì„± ì‹œì‘...")
        
        # âœ… ê¸°ë³¸ ë°©ë²• (langgraph 1.0.2)
        agent = create_react_agent(
            model=llm,
            tools=tools
        )
        
        logger.info("âœ… LangGraph ReAct Agent ìƒì„± ì™„ë£Œ")
        logger.warning("âš ï¸ System promptëŠ” messagesì— ì§ì ‘ ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤")
        return agent
        
    except Exception as e:
        logger.error(f"âŒ Agent ìƒì„± ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return None


# ============================================================
# ëŒ€í™” íˆìŠ¤í† ë¦¬ ë³€í™˜
# ============================================================

def convert_history_to_messages(history: List[Dict[str, str]]) -> List:
    """ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ LangChain Message í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    messages = []
    for msg in history[-5:]:
        if msg["role"] == "user":
            messages.append(HumanMessage(content=msg["content"]))
        elif msg["role"] == "ai":
            messages.append(AIMessage(content=msg["content"]))
    return messages


# ============================================================
# Agent ì‹¤í–‰ (System Prompt í¬í•¨)
# ============================================================

def run_agent(user_query: str, conversation_id: str, conversation_history: List[Dict[str, str]] = None) -> Dict[str, Any]:
    """LangGraph Agent ì‹¤í–‰"""
    logger.info(f"ğŸš€ Agent ì‹¤í–‰: conversation_id={conversation_id}")
    
    agent = create_langchain_agent()
    
    if agent is None:
        return {
            "answer": "ì£„ì†¡í•©ë‹ˆë‹¤. Agent ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. langgraph íŒ¨í‚¤ì§€ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.",
            "conversation_history": conversation_history or [],
            "tools_used": []
        }
    
    # System promptë¥¼ ì²« ë©”ì‹œì§€ë¡œ ì¶”ê°€
    from langchain_core.messages import SystemMessage
    
    system_prompt = """ë‹¹ì‹ ì€ ì•„ì´ì™€ í•¨ê»˜í•  ìˆ˜ ìˆëŠ” í™œë™ì„ ì¶”ì²œí•˜ëŠ” ì „ë¬¸ ì±—ë´‡ì…ë‹ˆë‹¤.

**ì¤‘ìš” ê·œì¹™:**
1. ìœ„ì¹˜ í™•ì¸ì´ ìµœìš°ì„ ì…ë‹ˆë‹¤
2. ìœ„ì¹˜ê°€ í™•ì¸ë˜ë©´ weather_toolê³¼ rag_search_tool ì‚¬ìš©
3. ê°ì • í‘œí˜„ì€ ë„êµ¬ ì—†ì´ ë°”ë¡œ ì‘ë‹µ

ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ë¥¼ í™œìš©í•˜ì—¬ ìµœì„ ì˜ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”."""
    
    # ë©”ì‹œì§€ êµ¬ì„±
    chat_history = convert_history_to_messages(conversation_history or [])
    
    # System promptë¥¼ ë§¨ ì•ì— ì¶”ê°€
    all_messages = [SystemMessage(content=system_prompt)] + chat_history + [HumanMessage(content=user_query)]
    
    try:
        result = agent.invoke({"messages": all_messages})
        answer = ""
        tools_used = []
        
        if "messages" in result:
            # ë§ˆì§€ë§‰ AI ë©”ì‹œì§€ ì°¾ê¸°
            for msg in reversed(result["messages"]):
                if isinstance(msg, AIMessage) and msg.content:
                    answer = msg.content
                    break
            
            # ì‚¬ìš©ëœ ë„êµ¬ ì¶”ì¶œ
            for msg in result["messages"]:
                if isinstance(msg, AIMessage) and hasattr(msg, "tool_calls") and msg.tool_calls:
                    for tool_call in msg.tool_calls:
                        if isinstance(tool_call, dict) and "name" in tool_call:
                            tools_used.append(tool_call["name"])
        
        if not answer:
            answer = "ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        new_history = (conversation_history or []) + [
            {"role": "user", "content": user_query},
            {"role": "ai", "content": answer},
        ]
        
        logger.info(f"âœ… Agent ì™„ë£Œ (ì‚¬ìš©ëœ ë„êµ¬: {list(set(tools_used))})")
        return {
            "answer": answer,
            "conversation_history": new_history,
            "tools_used": list(set(tools_used))
        }
    
    except Exception as e:
        logger.error(f"âŒ Agent ì‹¤í–‰ ì˜¤ë¥˜: {e}", exc_info=True)
        fallback_answer = "ì£„ì†¡í•©ë‹ˆë‹¤. ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”. ğŸ™"
        new_history = (conversation_history or []) + [
            {"role": "user", "content": user_query},
            {"role": "ai", "content": fallback_answer},
        ]
        return {
            "answer": fallback_answer,
            "conversation_history": new_history,
            "tools_used": []
        }