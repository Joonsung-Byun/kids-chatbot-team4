"""
LangChain Agent Service (LangChain 1.0+ with LangGraph)

LangGraphì˜ create_react_agentë¥¼ ì‚¬ìš©í•œ ë„êµ¬ ìë™ ì„ íƒ ë° ì‹¤í–‰
langgraph 1.0.2 ë²„ì „ í˜¸í™˜ + MockChatModel Runnable êµ¬í˜„
"""

import json
from typing import List, Dict, Any, Iterator

from langchain_core.tools import tool
from langchain_core.messages import HumanMessage, AIMessage, BaseMessage
from langchain_core.runnables import Runnable
from langchain_core.language_models import BaseChatModel
from langchain_core.outputs import ChatResult, ChatGeneration

from services.llm_service import get_llm_service
from services.rag_service import get_rag_service
from services.weather_service import get_weather
from services.map_service import get_map_markers
from utils.logger import logger
"""
ê°œì„ ëœ LangChain Agent Service
3ë‹¨ê³„ ì˜ì‚¬ê²°ì • ë¡œì§ ëª…í™•íˆ êµ¬í˜„:
1. ê°ì • í‘œí˜„ â†’ ì¦‰ì‹œ ì‘ë‹µ
2. ìœ„ì¹˜ ì •ë³´ ì—†ìŒ â†’ Multi-turn ì§ˆë¬¸
3. ìœ„ì¹˜ ì •ë³´ ìˆìŒ â†’ Weather + RAG í˜¸ì¶œ
"""

import json
import re
from typing import List, Dict, Any, Optional

from langchain_core.tools import tool
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, BaseMessage
from langchain_core.language_models import BaseChatModel
from langchain_core.outputs import ChatResult, ChatGeneration

from services.llm_service import get_llm_service
from services.rag_service import get_rag_service
from services.weather_service import get_weather
from utils.logger import logger


# ============================================================
# 1ë‹¨ê³„: ì¿¼ë¦¬ ë¶„ì„ (ê°ì •/ìœ„ì¹˜ ê°ì§€)
# ============================================================

def analyze_user_query(query: str, conversation_history: List[Dict[str, str]]) -> Dict[str, Any]:
    """
    ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ 3ë‹¨ê³„ë¡œ ë¶„ë¥˜
    
    Returns:
        {
            "type": "emotion" | "need_location" | "ready",
            "location": str or None,
            "date": str or None,
            "has_emotion": bool
        }
    """
    query_lower = query.lower()
    
    # 1ë‹¨ê³„: ê°ì • í‘œí˜„ ê°ì§€
    emotion_keywords = [
        "ê³ ë§ˆì›Œ", "ê°ì‚¬", "ì¢‹ì•„", "ìµœê³ ", "ì™„ë²½", "í›Œë¥­",
        "thank", "thanks", "great", "awesome", "perfect"
    ]
    has_emotion = any(keyword in query_lower for keyword in emotion_keywords)
    
    # 2ë‹¨ê³„: ìœ„ì¹˜ ì •ë³´ ì¶”ì¶œ
    location = extract_location(query)
    
    # ì´ì „ ëŒ€í™”ì—ì„œ ìœ„ì¹˜ ì°¾ê¸°
    if not location:
        location = extract_location_from_history(conversation_history)
    
    # 3ë‹¨ê³„: ë‚ ì§œ ì •ë³´ ì¶”ì¶œ
    date_info = extract_date(query)
    
    # ì¿¼ë¦¬ íƒ€ì… ê²°ì •
    if has_emotion and not any(k in query_lower for k in ["ì¶”ì²œ", "ì°¾ì•„", "ì–´ë””", "ë­í•´", "ê°ˆë§Œí•œ"]):
        return {"type": "emotion", "location": None, "date": None, "has_emotion": True}
    
    if not location:
        return {"type": "need_location", "location": None, "date": date_info, "has_emotion": False}
    
    return {"type": "ready", "location": location, "date": date_info, "has_emotion": False}


def extract_location(text: str) -> Optional[str]:
    """í…ìŠ¤íŠ¸ì—ì„œ ìœ„ì¹˜ ì •ë³´ ì¶”ì¶œ"""
    # ì‹œ/ë„ íŒ¨í„´
    cities = [
        "ì„œìš¸", "ë¶€ì‚°", "ëŒ€êµ¬", "ì¸ì²œ", "ê´‘ì£¼", "ëŒ€ì „", "ìš¸ì‚°", "ì„¸ì¢…",
        "ê²½ê¸°", "ê°•ì›", "ì¶©ë¶", "ì¶©ë‚¨", "ì „ë¶", "ì „ë‚¨", "ê²½ë¶", "ê²½ë‚¨", "ì œì£¼"
    ]
    
    # êµ¬/ë™ íŒ¨í„´
    districts = [
        "ê°•ë‚¨", "ì„œì´ˆ", "ì†¡íŒŒ", "ê°•ë™", "ë§ˆí¬", "ìš©ì‚°", "ì„±ë™", "ê´‘ì§„",
        "ì¤‘êµ¬", "ì¢…ë¡œ", "ì€í‰", "ì„œëŒ€ë¬¸", "ë™ëŒ€ë¬¸", "ì„±ë¶", "ê°•ë¶", "ë„ë´‰",
        "ë…¸ì›", "ë™ì‘", "ê´€ì•…", "ê¸ˆì²œ", "êµ¬ë¡œ", "ì˜ë“±í¬", "ì–‘ì²œ", "ê°•ì„œ"
    ]
    
    for city in cities:
        if city in text:
            # êµ¬ê¹Œì§€ ìˆëŠ”ì§€ í™•ì¸
            for district in districts:
                if district in text:
                    return f"{city} {district}"
            return city
    
    for district in districts:
        if district in text:
            return f"ì„œìš¸ {district}"  # ê¸°ë³¸ê°’ìœ¼ë¡œ ì„œìš¸ ì¶”ê°€
    
    return None


def extract_location_from_history(history: List[Dict[str, str]]) -> Optional[str]:
    """ëŒ€í™” íˆìŠ¤í† ë¦¬ì—ì„œ ìœ„ì¹˜ ì •ë³´ ì°¾ê¸°"""
    for msg in reversed(history[-10:]):  # ìµœê·¼ 10ê°œ ë©”ì‹œì§€ë§Œ
        if msg["role"] == "user":
            location = extract_location(msg["content"])
            if location:
                logger.info(f"ğŸ“ íˆìŠ¤í† ë¦¬ì—ì„œ ìœ„ì¹˜ ë°œê²¬: {location}")
                return location
    return None


def extract_date(text: str) -> Optional[str]:
    """ë‚ ì§œ ì •ë³´ ì¶”ì¶œ"""
    date_keywords = {
        "ì˜¤ëŠ˜": "today",
        "ë‚´ì¼": "tomorrow",
        "ëª¨ë ˆ": "day_after_tomorrow",
        "ì´ë²ˆì£¼": "this_week",
        "ë‹¤ìŒì£¼": "next_week",
        "ì£¼ë§": "weekend"
    }
    
    for keyword, value in date_keywords.items():
        if keyword in text:
            return value
    
    # YYYY-MM-DD í˜•ì‹ ì°¾ê¸°
    date_pattern = r'\d{4}-\d{2}-\d{2}'
    match = re.search(date_pattern, text)
    if match:
        return match.group()
    
    return None


# ============================================================
# 2ë‹¨ê³„: Tool ì •ì˜
# ============================================================

@tool
def weather_tool(location: str) -> str:
    """ë‚ ì”¨ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."""
    try:
        logger.info(f"[WeatherTool] í˜¸ì¶œ: {location}")
        result = get_weather(location=location, target_date=None)
        return json.dumps(result, ensure_ascii=False)
    except Exception as e:
        logger.error(f"[WeatherTool] ì˜¤ë¥˜: {e}")
        return json.dumps({"error": str(e)})


@tool
def rag_search_tool(query: str, location: Optional[str] = None) -> str:
    """ë¬¸í™”/ì²´ìœ¡/êµìœ¡ ì‹œì„¤ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    try:
        logger.info(f"[RAGTool] í˜¸ì¶œ: query='{query}', location='{location}'")
        rag_service = get_rag_service()
        
        # ìœ„ì¹˜ ê¸°ë°˜ í•„í„° êµ¬ì„±
        filters = {}
        if location:
            # "ì„œìš¸ ê°•ë‚¨" â†’ region_city="ì„œìš¸", region_gu="ê°•ë‚¨"
            parts = location.split()
            if len(parts) >= 2:
                filters["region_city"] = parts[0]
                filters["region_gu"] = parts[1]
            elif len(parts) == 1:
                filters["region_city"] = parts[0]
        
        results = rag_service.search_and_rerank(
            query=query, 
            top_k=5,
            filters=filters if filters else None
        )
        
        formatted = []
        for doc in results[:3]:
            metadata = doc.get("metadata", {})
            formatted.append({
                "name": metadata.get("facility_name", "Unknown"),
                "category": f"{metadata.get('category1', '')}-{metadata.get('category2', '')}",
                "location": f"{metadata.get('region_city', '')}/{metadata.get('region_gu', '')}",
                "price": metadata.get("price", "ë¬´ë£Œ"),
                "in_out": metadata.get("in_out", ""),
                "target_age": metadata.get("target_age", "")
            })
        return json.dumps(formatted, ensure_ascii=False)
    except Exception as e:
        logger.error(f"[RAGTool] ì˜¤ë¥˜: {e}")
        return json.dumps({"error": str(e)})


# ============================================================
# 3ë‹¨ê³„: Mock LLM (CPU í™˜ê²½ìš©)
# ============================================================

class MockChatModel(BaseChatModel):
    """CPU í™˜ê²½ìš© Mock ChatModel"""
    
    model_name: str = "mock-chat-model"
    
    def _generate(self, messages: List[BaseMessage], stop=None, **kwargs) -> ChatResult:
        user_input = ""
        for msg in reversed(messages):
            if isinstance(msg, HumanMessage):
                user_input = msg.content
                break
        
        # ì¿¼ë¦¬ ë¶„ì„
        analysis = analyze_user_query(user_input, [])
        
        if analysis["type"] == "emotion":
            response_text = "ì²œë§Œì—ìš”! ğŸ˜Š ë” ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ì£¼ì„¸ìš”!"
        elif analysis["type"] == "need_location":
            response_text = "ì–´ëŠ ì§€ì—­ì„ ìƒê°í•˜ê³  ê³„ì‹ ê°€ìš”? ğŸ—ºï¸ (ì„œìš¸, ë¶€ì‚°, ëŒ€êµ¬ ë“±)"
        else:
            response_text = f"Mock ëª¨ë“œì…ë‹ˆë‹¤. ìœ„ì¹˜({analysis['location']})ì™€ ë‚ ì§œ({analysis['date']})ë¥¼ í™•ì¸í–ˆìŠµë‹ˆë‹¤."
        
        message = AIMessage(content=response_text)
        generation = ChatGeneration(message=message)
        return ChatResult(generations=[generation])
    
    @property
    def _llm_type(self) -> str:
        return "mock-chat-model"
    
    def bind_tools(self, tools, **kwargs):
        return self


# ============================================================
# 4ë‹¨ê³„: Agent ì‹¤í–‰ í•¨ìˆ˜ (3ë‹¨ê³„ ë¡œì§ êµ¬í˜„)
# ============================================================

def run_agent(
    user_query: str, 
    conversation_id: str, 
    conversation_history: List[Dict[str, str]] = None
) -> Dict[str, Any]:
    """
    ê°œì„ ëœ Agent ì‹¤í–‰ ë¡œì§
    
    1ë‹¨ê³„: ì¿¼ë¦¬ ë¶„ì„
    2ë‹¨ê³„: íƒ€ì…ë³„ ì²˜ë¦¬
        - emotion: ì¦‰ì‹œ ì‘ë‹µ
        - need_location: ìœ„ì¹˜ ì§ˆë¬¸
        - ready: Weather + RAG í˜¸ì¶œ í›„ LLM ë‹µë³€ ìƒì„±
    """
    logger.info(f"ğŸš€ Agent ì‹¤í–‰: '{user_query}'")
    
    history = conversation_history or []
    
    # 1ë‹¨ê³„: ì¿¼ë¦¬ ë¶„ì„
    analysis = analyze_user_query(user_query, history)
    logger.info(f"ğŸ“Š ì¿¼ë¦¬ ë¶„ì„ ê²°ê³¼: {analysis}")
    
    # 2ë‹¨ê³„: íƒ€ì…ë³„ ì²˜ë¦¬
    if analysis["type"] == "emotion":
        # ê°ì • í‘œí˜„ â†’ ì¦‰ì‹œ ì‘ë‹µ
        answer = generate_emotion_response(user_query)
        tools_used = []
    
    elif analysis["type"] == "need_location":
        # ìœ„ì¹˜ ì •ë³´ ì—†ìŒ â†’ ì§ˆë¬¸
        answer = "ì–´ëŠ ì§€ì—­ì„ ìƒê°í•˜ê³  ê³„ì‹ ê°€ìš”? ğŸ—ºï¸\n(ì˜ˆ: ì„œìš¸ ê°•ë‚¨, ë¶€ì‚° í•´ìš´ëŒ€)"
        tools_used = []
    
    else:  # type == "ready"
        # ìœ„ì¹˜ ì •ë³´ ìˆìŒ â†’ Weather + RAG ì‹¤í–‰
        location = analysis["location"]
        date_info = analysis["date"]
        
        # Tool ì‹¤í–‰
        weather_result = call_weather_tool(location, date_info)
        rag_result = call_rag_tool(user_query, location)
        
        # LLMìœ¼ë¡œ ìµœì¢… ë‹µë³€ ìƒì„±
        answer = generate_final_answer(
            query=user_query,
            location=location,
            weather=weather_result,
            facilities=rag_result
        )
        tools_used = ["weather_tool", "rag_search_tool"]
    
    # íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
    new_history = history + [
        {"role": "user", "content": user_query},
        {"role": "ai", "content": answer}
    ]
    
    logger.info(f"âœ… Agent ì™„ë£Œ (ë„êµ¬: {tools_used})")
    
    return {
        "answer": answer,
        "conversation_history": new_history,
        "tools_used": tools_used,
        "query_analysis": analysis
    }


# ============================================================
# í—¬í¼ í•¨ìˆ˜ë“¤
# ============================================================

def generate_emotion_response(query: str) -> str:
    """ê°ì • í‘œí˜„ì— ëŒ€í•œ ì‘ë‹µ"""
    responses = {
        "ê³ ë§ˆ": "ì²œë§Œì—ìš”! ğŸ˜Š ë” ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ì£¼ì„¸ìš”!",
        "ê°ì‚¬": "ë„ì›€ì´ ë˜ì—ˆë‹¤ë‹ˆ ê¸°ì©ë‹ˆë‹¤! ğŸ‰ ë˜ í•„ìš”í•˜ì‹  ê²Œ ìˆìœ¼ì‹œë©´ ë§ì”€í•´ì£¼ì„¸ìš”!",
        "ì¢‹ì•„": "ë§ˆìŒì— ë“œì…¨ë‹¤ë‹ˆ ì •ë§ ê¸°ì©ë‹ˆë‹¤! ğŸ˜„ ì¦ê±°ìš´ ì‹œê°„ ë³´ë‚´ì„¸ìš”!",
        "ìµœê³ ": "ê°ì‚¬í•©ë‹ˆë‹¤! ğŸ˜Š í•­ìƒ ìµœì„ ì„ ë‹¤í•˜ê² ìŠµë‹ˆë‹¤!",
        "ì™„ë²½": "ì™„ë²½í•˜ë‹¤ëŠ” ë§ì”€ ê°ì‚¬í•©ë‹ˆë‹¤! âœ¨ ì¦ê±°ìš´ ì‹œê°„ ë˜ì„¸ìš”!"
    }
    
    for keyword, response in responses.items():
        if keyword in query:
            return response
    
    return "ë§ì”€í•´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤! ğŸ˜Š ë” ë„ì™€ë“œë¦´ ê²ƒì´ ìˆì„ê¹Œìš”?"


def call_weather_tool(location: str, date_info: Optional[str]) -> Dict[str, Any]:
    """ë‚ ì”¨ ë„êµ¬ í˜¸ì¶œ"""
    try:
        result = get_weather(location=location, target_date=date_info)
        logger.info(f"ğŸŒ¤ï¸ ë‚ ì”¨ ì¡°íšŒ ì™„ë£Œ: {location}")
        return result
    except Exception as e:
        logger.error(f"âŒ ë‚ ì”¨ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return {"location": location, "status": "sunny", "error": str(e)}


def call_rag_tool(query: str, location: str) -> List[Dict[str, Any]]:
    """RAG ë„êµ¬ í˜¸ì¶œ"""
    try:
        rag_service = get_rag_service()
        
        # í•„í„° êµ¬ì„±
        filters = {}
        parts = location.split()
        if len(parts) >= 2:
            filters["region_city"] = parts[0]
            filters["region_gu"] = parts[1]
        elif len(parts) == 1:
            filters["region_city"] = parts[0]
        
        results = rag_service.search_and_rerank(
            query=query,
            top_k=5,
            filters=filters if filters else None
        )
        
        logger.info(f"ğŸ” RAG ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ì‹œì„¤")
        return results
    except Exception as e:
        logger.error(f"âŒ RAG ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return []


def generate_final_answer(
    query: str,
    location: str,
    weather: Dict[str, Any],
    facilities: List[Dict[str, Any]]
) -> str:
    """ìµœì¢… ë‹µë³€ ìƒì„±"""
    llm_service = get_llm_service()
    
    # GPU í™˜ê²½: ì‹¤ì œ LLM ì‚¬ìš©
    if llm_service._use_gpu and llm_service._model:
        try:
            # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context = f"ìœ„ì¹˜: {location}\n"
            context += f"ë‚ ì”¨: {weather.get('status', 'ì•Œ ìˆ˜ ì—†ìŒ')}\n\n"
            context += "ì¶”ì²œ ì‹œì„¤:\n"
            
            for i, doc in enumerate(facilities[:3], 1):
                meta = doc.get("metadata", {})
                context += f"{i}. {meta.get('facility_name', 'N/A')}\n"
                context += f"   - ìœ„ì¹˜: {meta.get('region_gu', '')}\n"
                context += f"   - ë¶„ë¥˜: {meta.get('category1', '')}\n"
                context += f"   - ê°€ê²©: {meta.get('price', 'ë¬´ë£Œ')}\n\n"
            
            # LLM í”„ë¡¬í”„íŠ¸
            prompt = f"""ì•„ë˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

{context}

ì‚¬ìš©ì ì§ˆë¬¸: {query}

ë‹µë³€ ì‘ì„± ê°€ì´ë“œ:
- ë‚ ì”¨ ì •ë³´ë¥¼ ë¨¼ì € ì–¸ê¸‰
- ì¶”ì²œ ì‹œì„¤ 3ê°œë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì†Œê°œ
- ì´ëª¨ì§€ ì‚¬ìš© (ğŸ¨, ğŸƒâ€â™‚ï¸, ğŸ“)
- ë”°ëœ»í•˜ê³  ì¹œê·¼í•œ í†¤

ë‹µë³€:"""
            
            # í† í¬ë‚˜ì´ì§• ë° ìƒì„±
            from transformers import GenerationConfig
            inputs = llm_service._tokenizer(
                prompt,
                return_tensors="pt",
                truncation=True,
                max_length=1024
            ).to(llm_service._model.device)
            
            gen_cfg = GenerationConfig(
                temperature=0.7,
                max_new_tokens=300,
                top_p=0.9
            )
            
            import torch
            with torch.no_grad():
                out = llm_service._model.generate(**inputs, generation_config=gen_cfg)
            
            answer = llm_service._tokenizer.decode(out[0], skip_special_tokens=True)
            answer = answer.split("ë‹µë³€:")[-1].strip()
            
            return answer
        
        except Exception as e:
            logger.error(f"âŒ LLM ìƒì„± ì‹¤íŒ¨: {e}")
            # fallback to mock
    
    # Mock ëª¨ë“œ ë‹µë³€
    weather_status = weather.get("status", "ë§‘ìŒ")
    weather_emoji = {"sunny": "â˜€ï¸", "rainy": "ğŸŒ§ï¸", "cloudy": "â˜ï¸"}.get(weather_status, "ğŸŒ¤ï¸")
    
    answer = f"{weather_emoji} {location} ë‚ ì”¨ëŠ” {weather_status}ì´ì—ìš”!\n\n"
    answer += "ì¶”ì²œ ì¥ì†Œë¥¼ ì°¾ì•˜ì–´ìš”! ğŸ‰\n\n"
    
    for i, doc in enumerate(facilities[:3], 1):
        meta = doc.get("metadata", {})
        name = meta.get("facility_name", "Unknown")
        category = meta.get("category1", "ì‹œì„¤")
        gu = meta.get("region_gu", "")
        price = meta.get("price", "ë¬´ë£Œ")
        
        answer += f"{i}. ğŸ“ **{name}** ({gu})\n"
        answer += f"   ë¶„ë¥˜: {category} | ê°€ê²©: {price}\n\n"
    
    answer += "ì¦ê±°ìš´ ì‹œê°„ ë³´ë‚´ì„¸ìš”! ğŸ˜Š"
    
    return answer


# ============================================================
# get_tools í•¨ìˆ˜ (í˜¸í™˜ì„±)
# ============================================================

def get_tools():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡"""
    return [weather_tool, rag_search_tool]


# ============================================================
# Tool ì •ì˜ (LangGraph ë°©ì‹)
# ============================================================

@tool
def weather_tool(location: str) -> str:
    """ë‚ ì”¨ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤. ì…ë ¥: ì§€ì—­ëª… (ì˜ˆ: 'ì„œìš¸', 'ê°•ë‚¨')"""
    try:
        logger.info(f"[WeatherTool] í˜¸ì¶œ: {location}")
        result = get_weather(location=location, target_date=None)
        
        return json.dumps(result, ensure_ascii=False)
    except Exception as e:
        logger.error(f"[WeatherTool] ì˜¤ë¥˜: {e}")
        return json.dumps({"error": str(e)})


@tool
def rag_search_tool(query: str) -> str:
    """ë¬¸í™”/ì²´ìœ¡/êµìœ¡ ì‹œì„¤ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤. ì…ë ¥: ê²€ìƒ‰ ì¿¼ë¦¬ (ì˜ˆ: 'ì„œìš¸ ë†€ì´í„°', 'ê°•ë‚¨ í‚¤ì¦ˆì¹´í˜')"""
    try:
        logger.info(f"[RAGTool] í˜¸ì¶œ: {query}")
        rag_service = get_rag_service()
        results = rag_service.search_and_rerank(query=query, top_k=5)
        
        formatted = []
        for doc in results[:3]:
            metadata = doc.get("metadata", {})
            formatted.append({
                "name": metadata.get("facility_name", "Unknown"),
                "category": metadata.get("category1", "ì‹œì„¤"),
                "location": metadata.get("region_gu", ""),
                "price": metadata.get("price", "ë¬´ë£Œ")
            })
        return json.dumps(formatted, ensure_ascii=False)
    except Exception as e:
        logger.error(f"[RAGTool] ì˜¤ë¥˜: {e}")
        return json.dumps({"error": str(e)})


@tool
def map_tool(query: str) -> str:
    """ì§€ë„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì…ë ¥: ì‹œì„¤ ì •ë³´"""
    try:
        logger.info(f"[MapTool] í˜¸ì¶œ: {query}")
        # TODO: ì‹¤ì œ ì¹´ì¹´ì˜¤ë§µ API ì—°ë™
        return json.dumps({
            "status": "success",
            "message": "ì§€ë„ ìƒì„± ì™„ë£Œ (Mock)"
        }, ensure_ascii=False)
    except Exception as e:
        logger.error(f"[MapTool] ì˜¤ë¥˜: {e}")
        return json.dumps({"error": str(e)})


# ============================================================
# Tools ë¦¬ìŠ¤íŠ¸
# ============================================================

def get_tools():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡ ë°˜í™˜"""
    return [weather_tool, rag_search_tool, map_tool]


# ============================================================
# Mock LLM (CPU í™˜ê²½ìš©) - Runnable êµ¬í˜„
# ============================================================

class MockChatModel(BaseChatModel):
    """
    LangGraph í˜¸í™˜ Mock ChatModel
    BaseChatModelì„ ìƒì†í•˜ì—¬ Runnable ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„
    """
    
    model_name: str = "mock-chat-model"
    
    def _generate(self, messages: List[BaseMessage], stop=None, **kwargs) -> ChatResult:
        """í•„ìˆ˜ ë©”ì„œë“œ: ë©”ì‹œì§€ ìƒì„±"""
        # ë§ˆì§€ë§‰ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ì¶œ
        user_input = ""
        for msg in reversed(messages):
            if isinstance(msg, HumanMessage):
                user_input = msg.content
                break
        
        # ë£° ë² ì´ìŠ¤ ì‘ë‹µ
        if any(k in user_input for k in ["ì§€ì—­", "ì–´ë””", "ìœ„ì¹˜"]):
            response_text = "ì–´ëŠ ì§€ì—­ì„ ìƒê°í•˜ê³  ê³„ì‹ ê°€ìš”? ğŸ—ºï¸ (ì„œìš¸, ë¶€ì‚°, ëŒ€êµ¬ ë“±)"
        elif any(k in user_input for k in ["ê³ ë§ˆì›Œ", "ê°ì‚¬", "ì¢‹ì•„"]):
            response_text = "ì²œë§Œì—ìš”! ğŸ˜Š ë” ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ì£¼ì„¸ìš”!"
        else:
            response_text = "Mock ëª¨ë“œì…ë‹ˆë‹¤. ì‹¤ì œ LLMì„ ì‚¬ìš©í•˜ë ¤ë©´ OpenAI API í‚¤ë¥¼ ì„¤ì •í•˜ê±°ë‚˜ GPU í™˜ê²½ì—ì„œ ì‹¤í–‰í•´ì£¼ì„¸ìš”."
        
        # ChatResult ë°˜í™˜
        message = AIMessage(content=response_text)
        generation = ChatGeneration(message=message)
        return ChatResult(generations=[generation])
    
    @property
    def _llm_type(self) -> str:
        """í•„ìˆ˜ ì†ì„±: LLM íƒ€ì…"""
        return "mock-chat-model"
    
    def bind_tools(self, tools, **kwargs):
        """ë„êµ¬ ë°”ì¸ë”© (Mockì—ì„œëŠ” self ë°˜í™˜)"""
        return self
    
    def _stream(self, messages: List[BaseMessage], stop=None, **kwargs) -> Iterator[ChatResult]:
        """ì„ íƒ ë©”ì„œë“œ: ìŠ¤íŠ¸ë¦¬ë° (ë¯¸êµ¬í˜„)"""
        result = self._generate(messages, stop=stop, **kwargs)
        yield result
    
    async def _agenerate(self, messages: List[BaseMessage], stop=None, **kwargs) -> ChatResult:
        """ì„ íƒ ë©”ì„œë“œ: ë¹„ë™ê¸° ìƒì„± (ë™ê¸° ë²„ì „ ì¬ì‚¬ìš©)"""
        return self._generate(messages, stop=stop, **kwargs)


# ============================================================
# Agent ìƒì„±
# ============================================================

def create_langchain_agent():
    """LangGraph create_react_agentë¥¼ ì‚¬ìš©í•œ Agent ìƒì„±"""
    logger.info("ğŸ”§ LangGraph Agent ìƒì„± ì¤‘...")
    
    try:
        from langgraph.prebuilt import create_react_agent
    except ImportError:
        logger.error("âŒ langgraph íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install langgraph ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.")
        return None
    
    tools = get_tools()
    
    # System prompt ì •ì˜
    system_prompt = """ë‹¹ì‹ ì€ ì•„ì´ì™€ í•¨ê»˜í•  ìˆ˜ ìˆëŠ” í™œë™ì„ ì¶”ì²œí•˜ëŠ” ì „ë¬¸ ì±—ë´‡ì…ë‹ˆë‹¤.

**ì¤‘ìš” ê·œì¹™:**

1. **ìœ„ì¹˜ í™•ì¸ì´ ìµœìš°ì„ ì…ë‹ˆë‹¤:**
   - ì‚¬ìš©ì ì§ˆë¬¸ì— ì§€ì—­ëª…ì´ ì—†ìœ¼ë©´ ë¨¼ì € "ì–´ëŠ ì§€ì—­ì„ ìƒê°í•˜ê³  ê³„ì‹ ê°€ìš”? ğŸ—ºï¸" ì§ˆë¬¸
   - ì´ì „ ëŒ€í™”ì— ì§€ì—­ì´ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ì‚¬ìš©

2. **ìœ„ì¹˜ê°€ í™•ì¸ë˜ë©´ ë‹¤ìŒ ìˆœì„œë¡œ ì§„í–‰:**
   - Step 1: weather_toolë¡œ ë‚ ì”¨ í™•ì¸
   - Step 2: rag_search_toolë¡œ ì¶”ì²œ ì‹œì„¤ ê²€ìƒ‰
   - Step 3: ê²°ê³¼ë¥¼ ì¢…í•©í•´ì„œ ì¹œì ˆí•˜ê²Œ ë‹µë³€

3. **ê°ì • í‘œí˜„("ê³ ë§ˆì›Œ", "ì¢‹ì•„ìš”" ë“±):**
   - ë„êµ¬ ì‚¬ìš© ì—†ì´ ë°”ë¡œ ì¹œì ˆí•˜ê²Œ ì‘ë‹µ

4. **ë‹µë³€ ìŠ¤íƒ€ì¼:**
   - ì´ëª¨ì§€ ì‚¬ìš© (ğŸ¨, ğŸƒâ€â™‚ï¸, ğŸ“)
   - êµ¬ì²´ì ì¸ ì •ë³´ ì œê³µ
   - ì¶”ê°€ ì§ˆë¬¸ ìœ ë„

ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ë¥¼ í™œìš©í•˜ì—¬ ìµœì„ ì˜ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”."""
    
    # LLM ë¡œë“œ
    llm_service = get_llm_service()
    
    # OpenAI API í‚¤ê°€ ìˆìœ¼ë©´ OpenAI ì‚¬ìš©
    import os
    if os.getenv("OPENAI_API_KEY"):
        try:
            from langchain_openai import ChatOpenAI
            llm = ChatOpenAI(model="gpt-4o", temperature=0.7)
            logger.info("âœ… OpenAI LLM ì‚¬ìš©")
        except ImportError:
            logger.warning("âš ï¸ langchain-openai ë¯¸ì„¤ì¹˜ â†’ Mock ëª¨ë“œ")
            llm = MockChatModel()
    elif llm_service._use_gpu and llm_service._model:
        try:
            from langchain_huggingface import HuggingFacePipeline
            llm = HuggingFacePipeline(
                pipeline=llm_service._model,
                model_kwargs={"temperature": 0.7, "max_new_tokens": 512}
            )
            logger.info("âœ… GPU ëª¨ë“œ: HuggingFace LLM ì‚¬ìš©")
        except ImportError:
            logger.warning("âš ï¸ langchain-huggingface ë¯¸ì„¤ì¹˜ â†’ Mock ëª¨ë“œ")
            llm = MockChatModel()
    else:
        logger.info("âœ… CPU ëª¨ë“œ: Mock LLM ì‚¬ìš©")
        llm = MockChatModel()
    
    # ============================================================
    # langgraph ë²„ì „ë³„ í˜¸í™˜ì„± ì²˜ë¦¬
    # ============================================================
    
    # langgraph 1.0.2ëŠ” íŒŒë¼ë¯¸í„°ê°€ ê±°ì˜ ì—†ìŒ!
    # ê³µì‹ ë¬¸ì„œ: create_react_agent(model, tools, checkpointer=None)
    
    try:
        logger.info("ğŸ”§ Agent ìƒì„± ì‹œì‘...")
        
        # âœ… ê¸°ë³¸ ë°©ë²• (langgraph 1.0.2)
        agent = create_react_agent(
            model=llm,
            tools=tools
        )
        
        logger.info("âœ… LangGraph ReAct Agent ìƒì„± ì™„ë£Œ")
        logger.warning("âš ï¸ System promptëŠ” messagesì— ì§ì ‘ ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤")
        return agent
        
    except Exception as e:
        logger.error(f"âŒ Agent ìƒì„± ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return None


# ============================================================
# ëŒ€í™” íˆìŠ¤í† ë¦¬ ë³€í™˜
# ============================================================

def convert_history_to_messages(history: List[Dict[str, str]]) -> List:
    """ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ LangChain Message í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    messages = []
    for msg in history[-5:]:
        if msg["role"] == "user":
            messages.append(HumanMessage(content=msg["content"]))
        elif msg["role"] == "ai":
            messages.append(AIMessage(content=msg["content"]))
    return messages


# ============================================================
# Agent ì‹¤í–‰ (System Prompt í¬í•¨)
# ============================================================

def run_agent(user_query: str, conversation_id: str, conversation_history: List[Dict[str, str]] = None) -> Dict[str, Any]:
    """LangGraph Agent ì‹¤í–‰"""
    logger.info(f"ğŸš€ Agent ì‹¤í–‰: conversation_id={conversation_id}")
    
    agent = create_langchain_agent()
    
    if agent is None:
        return {
            "answer": "ì£„ì†¡í•©ë‹ˆë‹¤. Agent ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. langgraph íŒ¨í‚¤ì§€ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.",
            "conversation_history": conversation_history or [],
            "tools_used": []
        }
    
    # System promptë¥¼ ì²« ë©”ì‹œì§€ë¡œ ì¶”ê°€
    from langchain_core.messages import SystemMessage
    
    system_prompt = """ë‹¹ì‹ ì€ ì•„ì´ì™€ í•¨ê»˜í•  ìˆ˜ ìˆëŠ” í™œë™ì„ ì¶”ì²œí•˜ëŠ” ì „ë¬¸ ì±—ë´‡ì…ë‹ˆë‹¤.

**ì¤‘ìš” ê·œì¹™:**
1. ìœ„ì¹˜ í™•ì¸ì´ ìµœìš°ì„ ì…ë‹ˆë‹¤
2. ìœ„ì¹˜ê°€ í™•ì¸ë˜ë©´ weather_toolê³¼ rag_search_tool ì‚¬ìš©
3. ê°ì • í‘œí˜„ì€ ë„êµ¬ ì—†ì´ ë°”ë¡œ ì‘ë‹µ

ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ë¥¼ í™œìš©í•˜ì—¬ ìµœì„ ì˜ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”."""
    
    # ë©”ì‹œì§€ êµ¬ì„±
    chat_history = convert_history_to_messages(conversation_history or [])
    
    # System promptë¥¼ ë§¨ ì•ì— ì¶”ê°€
    all_messages = [SystemMessage(content=system_prompt)] + chat_history + [HumanMessage(content=user_query)]
    
    try:
        result = agent.invoke({"messages": all_messages})
        answer = ""
        tools_used = []
        
        if "messages" in result:
            # ë§ˆì§€ë§‰ AI ë©”ì‹œì§€ ì°¾ê¸°
            for msg in reversed(result["messages"]):
                if isinstance(msg, AIMessage) and msg.content:
                    answer = msg.content
                    break
            
            # ì‚¬ìš©ëœ ë„êµ¬ ì¶”ì¶œ
            for msg in result["messages"]:
                if isinstance(msg, AIMessage) and hasattr(msg, "tool_calls") and msg.tool_calls:
                    for tool_call in msg.tool_calls:
                        if isinstance(tool_call, dict) and "name" in tool_call:
                            tools_used.append(tool_call["name"])
        
        if not answer:
            answer = "ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        new_history = (conversation_history or []) + [
            {"role": "user", "content": user_query},
            {"role": "ai", "content": answer},
        ]
        
        logger.info(f"âœ… Agent ì™„ë£Œ (ì‚¬ìš©ëœ ë„êµ¬: {list(set(tools_used))})")
        return {
            "answer": answer,
            "conversation_history": new_history,
            "tools_used": list(set(tools_used))
        }
    
    except Exception as e:
        logger.error(f"âŒ Agent ì‹¤í–‰ ì˜¤ë¥˜: {e}", exc_info=True)
        fallback_answer = "ì£„ì†¡í•©ë‹ˆë‹¤. ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”. ğŸ™"
        new_history = (conversation_history or []) + [
            {"role": "user", "content": user_query},
            {"role": "ai", "content": fallback_answer},
        ]
        return {
            "answer": fallback_answer,
            "conversation_history": new_history,
            "tools_used": []
        }