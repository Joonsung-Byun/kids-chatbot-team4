"""
LLM Service - ë¡œì»¬ ëª¨ë¸ ê¸°ë°˜

ë¡œì»¬/GPU í™˜ê²½ì—ì„œ ì§ì ‘ ëª¨ë¸ ë¡œë”©
"""

from typing import List, Dict, Any
from utils.config import get_settings
from utils.logger import logger
import os


class LLMService:
    """LLM ê¸°ë°˜ ë‹µë³€ ìƒì„± ì„œë¹„ìŠ¤ (ë¡œì»¬ ëª¨ë¸)"""
    
    def __init__(self):
        self.settings = get_settings()
        self._llm_model = None
        self._tokenizer = None
        self._is_gpu_environment = self._detect_gpu_environment()
        
        if self._is_gpu_environment:
            self._load_llm_model()
    
    def _detect_gpu_environment(self) -> bool:
        """GPU í™˜ê²½ ê°ì§€"""
        try:
            if 'COLAB_RELEASE_TAG' in os.environ:
                return True
            import torch
            return torch.cuda.is_available()
        except ImportError:
            return False
    
    def _load_llm_model(self):
        """LLM ëª¨ë¸ ë¡œë“œ (GPU í™˜ê²½ì—ì„œë§Œ)"""
        try:
            # TODO: ì½”ëž©/RunPodì—ì„œ êµ¬í˜„ ì˜ˆì •
            # from transformers import AutoTokenizer, AutoModelForCausalLM
            # 
            # self._tokenizer = AutoTokenizer.from_pretrained(self.settings.GENERATION_MODEL)
            # self._llm_model = AutoModelForCausalLM.from_pretrained(
            #     self.settings.GENERATION_MODEL,
            #     device_map="auto",
            #     torch_dtype=torch.float16
            # )
            
            logger.info("ðŸ”„ LLM ëª¨ë¸ ë¡œë”© ì¤€ë¹„ë¨ (ì½”ëž©ì—ì„œ êµ¬í˜„ ì˜ˆì •)")
            
        except Exception as e:
            logger.error(f"LLM ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
    
    def generate_answer(self, query: str, context_docs: List[Dict[str, Any]]) -> str:
        """RAG ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë‹µë³€ ìƒì„±"""
        try:
            if not self._is_gpu_environment:
                # Mock ë‹µë³€ (ë¡œì»¬ ê°œë°œìš©)
                return self._generate_mock_answer(query, context_docs)
            
            # TODO: GPU í™˜ê²½ì—ì„œ ì‹¤ì œ LLM ì¶”ë¡ 
            # ì½”ëž©/RunPodì—ì„œ êµ¬í˜„ ì˜ˆì •
            logger.info(f"ðŸ¤– LLM ë‹µë³€ ìƒì„± (êµ¬í˜„ ì˜ˆì •): '{query}'")
            return "GPU í™˜ê²½ì—ì„œ LLM ë‹µë³€ ìƒì„± êµ¬í˜„ ì˜ˆì •ìž…ë‹ˆë‹¤."
            
        except Exception as e:
            logger.error(f"ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    def _generate_mock_answer(self, query: str, context_docs: List[Dict[str, Any]]) -> str:
        """Mock ë‹µë³€ ìƒì„± (ë¡œì»¬ ê°œë°œìš©)"""
        if not context_docs:
            return "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•´ë³´ì„¸ìš”."
        
        # ê°„ë‹¨í•œ í…œí”Œë¦¿ ê¸°ë°˜ ë‹µë³€
        facilities = [doc['metadata'].get('facility_name', 'Unknown') for doc in context_docs[:3]]
        
        return f"""
{query}ì— ëŒ€í•œ ì¶”ì²œ ê²°ê³¼ìž…ë‹ˆë‹¤:

ðŸŽ¯ ì¶”ì²œ ì‹œì„¤:
{chr(10).join([f"â€¢ {facility}" for facility in facilities])}

ì´ {len(context_docs)}ê°œì˜ ê´€ë ¨ ì‹œì„¤ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.
ë” ìžì„¸í•œ ì •ë³´ëŠ” ê° ì‹œì„¤ì— ì§ì ‘ ë¬¸ì˜í•´ë³´ì„¸ìš”!

(ì°¸ê³ : í˜„ìž¬ Mock ë‹µë³€ìž…ë‹ˆë‹¤. GPU í™˜ê²½ì—ì„œ ê³ í’ˆì§ˆ ë‹µë³€ì´ ìƒì„±ë©ë‹ˆë‹¤.)
        """.strip()
    
    def generate_clarifying_question(self, query: str, missing_info: List[str]) -> str:
        """ì—­ì§ˆë¬¸ ìƒì„±"""
        if not missing_info:
            return "ë” êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ ì•Œë ¤ì£¼ì‹œë©´ ë” ì •í™•í•œ ì¶”ì²œì„ ë“œë¦´ ìˆ˜ ìžˆì–´ìš”!"
        
        return f"ë” ì •í™•í•œ ì¶”ì²œì„ ìœ„í•´ {', '.join(missing_info)}ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”."


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤  
_llm_service_instance = None

def get_llm_service() -> LLMService:
    """LLM Service ì‹±ê¸€í†¤ ë°˜í™˜"""
    global _llm_service_instance
    if _llm_service_instance is None:
        _llm_service_instance = LLMService()
    return _llm_service_instance