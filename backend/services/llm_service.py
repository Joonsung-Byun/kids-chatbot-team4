# services/llm_service.py
"""
LLM Service

- GPU í™˜ê²½: ì‹¤ì œ ëª¨ë¸ ë¡œë”© ë° ì¶”ë¡ 
- CPU/Mock í™˜ê²½: ê°„ë‹¨í•œ Mock ë‹µë³€ ë°˜í™˜
"""

import os
from typing import List, Dict, Any

from utils.config import get_settings
from utils.logger import logger

# GPU ì „ìš© ë¼ì´ë¸ŒëŸ¬ë¦¬ import ì‹œë„
try:
    import torch
    from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig
except ImportError:
    torch = None
    AutoTokenizer = None
    AutoModelForCausalLM = None
    GenerationConfig = None


class LLMService:
    """LLM ê¸°ë°˜ ë‹µë³€ ìƒì„± ì„œë¹„ìŠ¤"""

    def __init__(self) -> None:
        self.settings = get_settings()
        self._tokenizer = None
        self._model = None
        self._use_gpu = self._detect_gpu()

        if self._use_gpu:
            self._load_model()
        else:
            logger.info("ðŸ”„ GPU ë¯¸ê²€ì¶œ ë˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¯¸ì„¤ì¹˜ â†’ Mock ëª¨ë“œë¡œ ë™ìž‘")

    def _detect_gpu(self) -> bool:
        """GPU í™˜ê²½ ê°ì§€ (Colab, CUDA ë“±)"""
        if os.getenv("COLAB_RELEASE_TAG"):
            return True
        if AutoModelForCausalLM and torch and torch.cuda.is_available():
            return True
        return False

    def _load_model(self) -> None:
        """GPU í™˜ê²½ì—ì„œ ì‹¤ì œ LLM ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ"""
        try:
            model_name = self.settings.GENERATION_MODEL
            logger.info(f"ðŸ”„ LLM ëª¨ë¸ ë¡œë”©: {model_name}")
            self._tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)
            self._model = AutoModelForCausalLM.from_pretrained(
                model_name, device_map="auto", torch_dtype=torch.float16, trust_remote_code=True
            )
            self._model.eval()
            logger.info("âœ… LLM ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ LLM ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self._use_gpu = False  # fallback to Mock
    
    # def generate_short_response(
    #     self,
    #     prompt: str,
    #     max_tokens: int = 100
    # ) -> str:
    #     """
    #     analyze_query_with_llm ì—ì„œ JSON íŒŒì‹±ìš©ìœ¼ë¡œ í˜¸ì¶œí•˜ëŠ” ë‹¨ë¬¸ ìƒì„±ê¸°.
    #     """
    #     # GPU í™˜ê²½ ì‹¤ì œ ëª¨ë¸ í˜¸ì¶œ
    #     if self._use_gpu and self._model and self._tokenizer:
    #         from transformers import GenerationConfig
    #         inputs = self._tokenizer(
    #             prompt,
    #             return_tensors="pt",
    #             truncation=True,
    #             max_length=512
    #         ).to(self._model.device)
    #         gen_cfg = GenerationConfig(
    #             temperature=0.7,
    #             max_new_tokens=max_tokens,
    #             top_p=0.9
    #         )
    #         with torch.no_grad():
    #             out = self._model.generate(**inputs, generation_config=gen_cfg)
    #         return self._tokenizer.decode(out[0], skip_special_tokens=True).strip()

    #     # Mock ëª¨ë“œ: ìµœì†Œí•œ ë¹ˆ JSONì´ë¼ë„ ë°˜í™˜
    #     return "{}"
    
    def generate_answer(
        self,
        query: str,
        context_docs: List[Dict[str, Any]],
    ) -> str:
        """
        RAG ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë‹µë³€ ìƒì„±
    
        Returns:
          - ì‹¤ì œ GPU í™˜ê²½: ëª¨ë¸ ì¶”ë¡  ê²°ê³¼
          - Mock í™˜ê²½: ê°„ë‹¨í•œ ì¶”ì²œ ë¦¬ìŠ¤íŠ¸
        """
        if self._use_gpu and self._model and self._tokenizer:
            try:
                # 1) Prompt ì¡°í•©
                context = "\n".join(doc["content"] for doc in context_docs)
                prompt = f"Context:\n{context}\n\nQuestion: {query}\nAnswer:"
                
                # 2) í† í¬ë‚˜ì´ì¦ˆ ë° Tensor ë³€í™˜
                inputs = self._tokenizer(
                    prompt, return_tensors="pt", truncation=True, max_length=1024
                ).to(self._model.device)
                
                # 3) ìƒì„± ì„¤ì •
                gen_cfg = GenerationConfig(temperature=0.7, max_new_tokens=256, top_p=0.9)
                
                # 4) ì¶”ë¡ 
                with torch.no_grad():
                    out = self._model.generate(**inputs, generation_config=gen_cfg)
                text = self._tokenizer.decode(out[0], skip_special_tokens=True)
                return text.split("Answer:")[-1].strip()
            except Exception as e:
                logger.error(f"âŒ LLM ì¶”ë¡  ì¤‘ ì˜¤ë¥˜: {e}")
                return self._mock_answer(query, context_docs)
        
        # Mock ëª¨ë“œ
        return self._mock_answer(query, context_docs)

    def _mock_answer(
        self,
        query: str,
        context_docs: List[Dict[str, Any]],
    ) -> str:
        """ê°œë°œìš© Mock ë‹µë³€ ìƒì„± (ê°œì„  ë²„ì „)"""
        if not context_docs:
            return "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•´ ë³´ì„¸ìš”."
        
        # ìƒìœ„ 3ê°œ ì‹œì„¤ ì •ë³´ ì¶”ì¶œ
        recommendations = []
        for doc in context_docs[:3]:
            metadata = doc.get("metadata", {})
            name = metadata.get("facility_name", "Unknown")
            category = metadata.get("category1", "ì‹œì„¤")
            location = metadata.get("region_gu", metadata.get("region_city", ""))
            price = metadata.get("price", "ë¬´ë£Œ")
            
            recommendations.append(
                f"ðŸ“ **{name}** ({location})\n"
                f"   ë¶„ë¥˜: {category} | ê°€ê²©: {price}"
            )
        
        answer = f"ì¶”ì²œ ìž¥ì†Œë¥¼ ì°¾ì•˜ì–´ìš”! ðŸŽ‰\n\n"
        answer += "\n\n".join(recommendations)
        answer += f"\n\nì´ {len(context_docs)}ê°œì˜ ìž¥ì†Œê°€ ìžˆì–´ìš”. ë” ê¶ê¸ˆí•œ ì ì´ ìžˆìœ¼ì‹œë©´ ë§ì”€í•´ì£¼ì„¸ìš”! ðŸ˜Š"
        answer += "\n\nðŸ’¡ *í˜„ìž¬ Mock ëª¨ë“œë¡œ ìž‘ë™ ì¤‘ìž…ë‹ˆë‹¤. GPU í™˜ê²½ì—ì„œëŠ” ë” ìžì„¸í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.*"
        
        return answer

    def generate_clarifying_question(
        self, query: str, missing_info: List[str]
    ) -> str:
        """
        ì‚¬ìš©ìžê°€ ë¹ ëœ¨ë¦° ì •ë³´ì— ëŒ€í•œ ì¶”ê°€ ì§ˆë¬¸ ìƒì„±
        """
        if not missing_info:
            return "ë” êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ ì•Œë ¤ì£¼ì‹œë©´ ë” ì •í™•í•œ ì¶”ì²œì„ ë“œë¦´ ìˆ˜ ìžˆì–´ìš”!"
        return f"ì¶”ê°€ë¡œ {', '.join(missing_info)} ì •ë³´ë¥¼ ì•Œë ¤ì£¼ì‹¤ ìˆ˜ ìžˆë‚˜ìš”?"


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_llm_service: LLMService = None


def get_llm_service() -> LLMService:
    """LLMService ì‹±ê¸€í†¤ ë°˜í™˜"""
    global _llm_service
    if _llm_service is None:
        _llm_service = LLMService()
    return _llm_service