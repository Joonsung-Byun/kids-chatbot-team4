# services/llm_service.py
"""
LLM Service

- GPU í™˜ê²½: ì‹¤ì œ ëª¨ë¸ ë¡œë”© ë° ì¶”ë¡ 
- CPU/Mock í™˜ê²½: ê°„ë‹¨í•œ Mock ë‹µë³€ ë°˜í™˜
"""

import os
from typing import List, Dict, Any

from utils.config import get_settings
from utils.logger import logger

# GPU ì „ìš© ë¼ì´ë¸ŒëŸ¬ë¦¬ import ì‹œë„
try:
    import torch
    from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig
except ImportError:
    torch = None
    AutoTokenizer = None
    AutoModelForCausalLM = None
    GenerationConfig = None


class LLMService:
    """LLM ê¸°ë°˜ ë‹µë³€ ìƒì„± ì„œë¹„ìŠ¤"""

    def __init__(self) -> None:
        self.settings = get_settings()
        self._tokenizer = None
        self._model = None
        self._use_gpu = self._detect_gpu()

        if self._use_gpu:
            self._load_model()
        else:
            logger.info("ðŸ”„ GPU ë¯¸ê²€ì¶œ ë˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¯¸ì„¤ì¹˜ â†’ Mock ëª¨ë“œë¡œ ë™ìž‘")

    def _detect_gpu(self) -> bool:
        """GPU í™˜ê²½ ê°ì§€ (Colab, CUDA ë“±)"""
        if os.getenv("COLAB_RELEASE_TAG"):
            return True
        if AutoModelForCausalLM and torch and torch.cuda.is_available():
            return True
        return False

    def _load_model(self) -> None:
        """GPU í™˜ê²½ì—ì„œ ì‹¤ì œ LLM ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ"""
        try:
            model_name = self.settings.GENERATION_MODEL
            logger.info(f"ðŸ”„ LLM ëª¨ë¸ ë¡œë”©: {model_name}")
            self._tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)
            self._model = AutoModelForCausalLM.from_pretrained(
                model_name, device_map="auto", torch_dtype=torch.float16, trust_remote_code=True
            )
            self._model.eval()
            logger.info("âœ… LLM ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ LLM ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self._use_gpu = False  # fallback to Mock

    def generate_answer(
        self,
        query: str,
        context_docs: List[Dict[str, Any]],
    ) -> str:
        """
        RAG ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë‹µë³€ ìƒì„±
    
        Returns:
          - ì‹¤ì œ GPU í™˜ê²½: ëª¨ë¸ ì¶”ë¡  ê²°ê³¼
          - Mock í™˜ê²½: ê°„ë‹¨í•œ ì¶”ì²œ ë¦¬ìŠ¤íŠ¸
        """
        if self._use_gpu and self._model and self._tokenizer:
            try:
                # 1) Prompt ì¡°í•©
                context = "\n".join(doc["content"] for doc in context_docs)
                prompt = f"Context:\n{context}\n\nQuestion: {query}\nAnswer:"
                # 2) í† í¬ë‚˜ì´ì¦ˆ ë° Tensor ë³€í™˜
                inputs = self._tokenizer(
                    prompt, return_tensors="pt", truncation=True, max_length=1024
                ).to(self._model.device)
                # 3) ìƒì„± ì„¤ì •
                gen_cfg = GenerationConfig(temperature=0.7, max_new_tokens=256, top_p=0.9)
                # 4) ì¶”ë¡ 
                with torch.no_grad():
                    out = self._model.generate(**inputs, generation_config=gen_cfg)
                text = self._tokenizer.decode(out[0], skip_special_tokens=True)
                return text.split("Answer:")[-1].strip()
            except Exception as e:
                logger.error(f"âŒ LLM ì¶”ë¡  ì¤‘ ì˜¤ë¥˜: {e}")
                return self._mock_answer(query, context_docs)
        # Mock ëª¨ë“œ
        return self._mock_answer(query, context_docs)

    def _mock_answer(
        self,
        query: str,
        context_docs: List[Dict[str, Any]],
    ) -> str:
        """ê°œë°œìš© Mock ë‹µë³€ ìƒì„±"""
        if not context_docs:
            return "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•´ ë³´ì„¸ìš”."
        facilities = [
            doc["metadata"].get("facility_name", "Unknown") for doc in context_docs[:3]
        ]
        items = "\n".join(f"â€¢ {name}" for name in facilities)
        return (
            f"{query}ì— ëŒ€í•œ ì¶”ì²œ ê²°ê³¼ìž…ë‹ˆë‹¤:\n"
            f"{items}\n"
            f"(ì´ {len(context_docs)}ê°œ, í˜„ìž¬ Mock ëª¨ë“œ)"
        )

    def generate_clarifying_question(
        self, query: str, missing_info: List[str]
    ) -> str:
        """
        ì‚¬ìš©ìžê°€ ë¹ ëœ¨ë¦° ì •ë³´ì— ëŒ€í•œ ì¶”ê°€ ì§ˆë¬¸ ìƒì„±
        """
        if not missing_info:
            return "ë” êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ ì•Œë ¤ì£¼ì‹œë©´ ë” ì •í™•í•œ ì¶”ì²œì„ ë“œë¦´ ìˆ˜ ìžˆì–´ìš”!"
        return f"ì¶”ê°€ë¡œ {', '.join(missing_info)} ì •ë³´ë¥¼ ì•Œë ¤ì£¼ì‹¤ ìˆ˜ ìžˆë‚˜ìš”?"


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_llm_service: LLMService = None


def get_llm_service() -> LLMService:
    """LLMService ì‹±ê¸€í†¤ ë°˜í™˜"""
    global _llm_service
    if _llm_service is None:
        _llm_service = LLMService()
    return _llm_service