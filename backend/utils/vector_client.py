"""
ChromaDB Cloud Vector Client

ì´ ëª¨ë“ˆì€ ChromaDB Cloudì™€ì˜ ì—°ê²° ë° ë²¡í„° ê²€ìƒ‰ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.
RAG íŒŒì´í”„ë¼ì¸ì—ì„œ ì‹œì„¤ ì •ë³´ë¥¼ ê²€ìƒ‰í•  ë•Œ ì‚¬ìš©ë©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- ChromaDB Cloud ì—°ê²° ê´€ë¦¬
- í…ìŠ¤íŠ¸ ê¸°ë°˜ ë²¡í„° ê²€ìƒ‰ (í™˜ê²½ë³„ ì„ë² ë”© ì²˜ë¦¬)
- ë©”íƒ€ë°ì´í„° í•„í„°ë§ ì§€ì›
- ì‹±ê¸€í†¤ íŒ¨í„´ìœ¼ë¡œ ì—°ê²° ì¬ì‚¬ìš©
- ë¡œì»¬/ì½”ë©/RunPod í™˜ê²½ ìë™ ê°ì§€

í™˜ê²½ë³„ ì„ë² ë”© ì²˜ë¦¬:
- ë¡œì»¬ CPU: Mock ì„ë² ë”© (ê°œë°œìš©)
- ì½”ë©/RunPod GPU: ì‹¤ì œ ëª¨ë¸ (sentence-transformers)

ì‚¬ìš© ì˜ˆì‹œ:
    from utils.vector_client import get_vector_client
    
    client = get_vector_client()
    results = client.search("í•œë‚¨ë™ ë†€ì´í„°", n_results=5)
"""

import os
import hashlib
from typing import List, Dict, Any, Optional

import chromadb
import numpy as np

from utils.config import get_settings
from utils.logger import logger


class VectorClient:
    """
    ChromaDB Cloud í´ë¼ì´ì–¸íŠ¸
    
    ì´ í´ë˜ìŠ¤ëŠ” ChromaDB Cloudì™€ì˜ ì—°ê²°ì„ ê´€ë¦¬í•˜ê³  ë²¡í„° ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    ì‹±ê¸€í†¤ íŒ¨í„´ì„ ì‚¬ìš©í•˜ì—¬ ì• í”Œë¦¬ì¼€ì´ì…˜ ì „ì²´ì—ì„œ í•˜ë‚˜ì˜ ì—°ê²°ë§Œ ìœ ì§€í•©ë‹ˆë‹¤.
    
    í™˜ê²½ ê°ì§€ ê¸°ëŠ¥:
    - ë¡œì»¬ ê°œë°œ: Mock ì„ë² ë”©ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
    - ì½”ë©/GPU: ì‹¤ì œ sentence-transformers ëª¨ë¸ ì‚¬ìš©
    
    Attributes:
        settings: í™˜ê²½ë³€ìˆ˜ ì„¤ì • ê°ì²´
        client: ChromaDB Cloud í´ë¼ì´ì–¸íŠ¸ ì¸ìŠ¤í„´ìŠ¤
        collection: ì‚¬ìš© ì¤‘ì¸ ChromaDB ì»¬ë ‰ì…˜
        _embedding_model: ë¡œë“œëœ ì„ë² ë”© ëª¨ë¸ (GPU í™˜ê²½ì—ì„œë§Œ)
        _is_gpu_environment: GPU í™˜ê²½ ì—¬ë¶€
    """
    
    def __init__(self):
        """
        VectorClient ì´ˆê¸°í™”
        
        í™˜ê²½ë³€ìˆ˜ë¥¼ ë¡œë“œí•˜ê³  ChromaDB Cloudì— ìë™ìœ¼ë¡œ ì—°ê²°í•©ë‹ˆë‹¤.
        í™˜ê²½ì„ ê°ì§€í•˜ì—¬ ì ì ˆí•œ ì„ë² ë”© ë°©ì‹ì„ ì„ íƒí•©ë‹ˆë‹¤.
        """
        self.settings = get_settings()
        self.client = None
        self.collection = None
        self._embedding_model = None
        self._is_gpu_environment = self._detect_environment()
        
        # ChromaDB ì—°ê²°
        self._connect()
        
        # GPU í™˜ê²½ì´ë©´ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
        if self._is_gpu_environment:
            self._load_embedding_model()
    
    def _detect_environment(self) -> bool:
        """
        í˜„ì¬ í™˜ê²½ ê°ì§€ (GPU/ì½”ë© vs ë¡œì»¬ CPU)
        
        Returns:
            bool: GPU í™˜ê²½ì´ë©´ True, ë¡œì»¬ CPUë©´ False
        """
        try:
            if "COLAB_RELEASE_TAG" in os.environ:
                logger.info("ğŸ” ì½”ë© í™˜ê²½ ê°ì§€ë¨")
                return True
            import torch
            if torch.cuda.is_available():
                logger.info("ğŸ” GPU í™˜ê²½ ê°ì§€ë¨")
                return True
        except ImportError:
            logger.info("ğŸ” torch ë¯¸ì„¤ì¹˜ - ë¡œì»¬ CPU í™˜ê²½ìœ¼ë¡œ íŒë‹¨")
        logger.info("ğŸ” CPU í™˜ê²½ ê°ì§€ë¨ (Mock ì„ë² ë”© ì‚¬ìš©)")
        return False
    
    def _connect(self):
        """
        ChromaDB Cloud ì—°ê²°
        
        í™˜ê²½ë³€ìˆ˜ì— ì„¤ì •ëœ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì—¬ ChromaDB Cloudì— ì—°ê²°í•˜ê³ ,
        ì§€ì •ëœ ì»¬ë ‰ì…˜ì„ ë¡œë“œí•©ë‹ˆë‹¤.
        
        ì—°ê²° ì •ë³´:
        - API Key: ì¸ì¦ìš© í‚¤ (CHROMA_API_KEY)
        - Tenant: í…Œë„ŒíŠ¸ ID (CHROMA_TENANT)
        - Database: ë°ì´í„°ë² ì´ìŠ¤ ì´ë¦„ (CHROMA_DATABASE)
        - Collection: ì»¬ë ‰ì…˜ ì´ë¦„ (CHROMA_COLLECTION_NAME)
        
        Raises:
            Exception: ì—°ê²° ì‹¤íŒ¨ ì‹œ (ì˜ëª»ëœ ì¸ì¦ ì •ë³´, ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ë“±)
        """
        try:
            logger.info("ChromaDB Cloud ì—°ê²° ì‹œë„...")
            self.client = chromadb.CloudClient(
                api_key=self.settings.CHROMA_API_KEY,
                tenant=self.settings.CHROMA_TENANT,
                database=self.settings.CHROMA_DATABASE,
            )
            self.collection = self.client.get_collection(
                name=self.settings.CHROMA_COLLECTION_NAME
            )
            logger.info(f"âœ… ChromaDB ì—°ê²° ì„±ê³µ: {self.collection.name} ({self.collection.count()}ê°œ)")
        except Exception as e:
            logger.error(f"âŒ ChromaDB ì—°ê²° ì‹¤íŒ¨: {e}")
            raise
    
    def _load_embedding_model(self):
        """
        ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (GPU í™˜ê²½ì—ì„œë§Œ)
        
        sentence-transformersë¥¼ ì‚¬ìš©í•˜ì—¬ íŒ€ì›ì´ ì‚¬ìš©í•œ ê²ƒê³¼ ë™ì¼í•œ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.
        ëª¨ë¸ì€ í•œ ë²ˆë§Œ ë¡œë“œë˜ì–´ ë©”ëª¨ë¦¬ì— ìºì‹œë©ë‹ˆë‹¤.
        """
        try:
            from sentence_transformers import SentenceTransformer
            
            logger.info(f"ğŸ”„ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘: {self.settings.EMBEDDING_MODEL}")
            
            # GPUì—ì„œ ëª¨ë¸ ë¡œë“œ
            self._embedding_model = SentenceTransformer(
                self.settings.EMBEDDING_MODEL,
                device='cuda' if self._is_gpu_environment else 'cpu'
            )
            
            logger.info("âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            logger.warning("Mock ì„ë² ë”©ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤")
            self._is_gpu_environment = False
            self._embedding_model = None
    
    def search(
        self,
        query_text: str,
        n_results: int = 10,
        where: Optional[Dict[str, Any]] = None,
        where_document: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰
        
        ì‚¬ìš©ìì˜ í…ìŠ¤íŠ¸ ì¿¼ë¦¬ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
        í™˜ê²½ì— ë”°ë¼ ì‹¤ì œ ëª¨ë¸ ë˜ëŠ” Mock ì„ë² ë”©ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
        
        ê²€ìƒ‰ ê³¼ì •:
        1. query_textë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜ (í™˜ê²½ë³„ ì²˜ë¦¬)
        2. ì €ì¥ëœ ë²¡í„°ë“¤ê³¼ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        3. ê°€ì¥ ìœ ì‚¬í•œ n_resultsê°œ ë°˜í™˜
        
        Args:
            query_text (str): ê²€ìƒ‰í•  í…ìŠ¤íŠ¸ ì¿¼ë¦¬
                ì˜ˆ: "í•œë‚¨ë™ ê·¼ì²˜ ë†€ì´í„°", "ì•„ì´ì™€ ê°ˆë§Œí•œ ë°•ë¬¼ê´€"
            
            n_results (int, optional): ë°˜í™˜í•  ê²°ê³¼ ê°œìˆ˜. ê¸°ë³¸ê°’ 10.
                TOP_Kë¡œ ë§ì´ ê°€ì ¸ì˜¨ í›„ Rerankingí•˜ëŠ” ê²½ìš° í¬ê²Œ ì„¤ì • (ì˜ˆ: 30)
            
            where (Dict[str, Any], optional): ë©”íƒ€ë°ì´í„° í•„í„° ì¡°ê±´
                ì˜ˆ: {"category": "ë†€ì´í„°"}, {"region": "ì„œìš¸"}
                ë³µí•© ì¡°ê±´: {"$and": [{"category": "ë†€ì´í„°"}, {"region": "ì„œìš¸"}]}
            
            where_document (Dict[str, Any], optional): ë¬¸ì„œ ë‚´ìš© í•„í„° ì¡°ê±´
                ì˜ˆ: {"$contains": "ë¬´ë£Œ"}
        
        Returns:
            Dict[str, Any]: ê²€ìƒ‰ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
                {
                    'ids': [[ë¬¸ì„œID1, ë¬¸ì„œID2, ...]],  # ê° ê²°ê³¼ì˜ ê³ ìœ  ID
                    'documents': [[ë¬¸ì„œ1, ë¬¸ì„œ2, ...]],  # ì‹¤ì œ í…ìŠ¤íŠ¸ ë‚´ìš©
                    'metadatas': [[ë©”íƒ€1, ë©”íƒ€2, ...]],  # ë©”íƒ€ë°ì´í„° (name, address ë“±)
                    'distances': [[ê±°ë¦¬1, ê±°ë¦¬2, ...]]  # ì½”ì‚¬ì¸ ê±°ë¦¬ (ë‚®ì„ìˆ˜ë¡ ìœ ì‚¬)
                }
                
                ì£¼ì˜: ëª¨ë“  ê°’ì´ ì´ì¤‘ ë¦¬ìŠ¤íŠ¸ [[...]] í˜•íƒœ
                ì²« ë²ˆì§¸ ë¦¬ìŠ¤íŠ¸ëŠ” ì¿¼ë¦¬ ê°œìˆ˜, ë‘ ë²ˆì§¸ëŠ” ê²°ê³¼ ê°œìˆ˜
        
        Raises:
            Exception: ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ (ì—°ê²° ëŠê¹€, ì˜ëª»ëœ í•„í„° ë“±)
        
        Examples:
            >>> client = get_vector_client()
            >>> 
            >>> # ê¸°ë³¸ ê²€ìƒ‰
            >>> results = client.search("ë†€ì´í„°", n_results=5)
            >>> 
            >>> # ë©”íƒ€ë°ì´í„° í•„í„° ì‚¬ìš©
            >>> results = client.search(
            ...     "ë†€ì´í„°",
            ...     n_results=5,
            ...     where={"region": "ì„œìš¸"}
            ... )
            >>> 
            >>> # ê²°ê³¼ ì ‘ê·¼
            >>> for doc, meta, dist in zip(
            ...     results['documents'][0],
            ...     results['metadatas'][0],
            ...     results['distances'][0]
            ... ):
            ...     print(f"ì´ë¦„: {meta['name']}, ê±°ë¦¬: {dist}")
        """
        try:
            env_status = "GPU" if self._is_gpu_environment else "Mock"
            logger.info(f"ğŸ” ê²€ìƒ‰ ì¿¼ë¦¬: '{query_text}' (n_results={n_results}, í™˜ê²½={env_status})")
            
            # where í•„í„° ë¡œê·¸ (ë””ë²„ê¹…ìš©)
            if where:
                logger.info(f"   ë©”íƒ€ë°ì´í„° í•„í„°: {where}")
            
            # ì¿¼ë¦¬ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜
            query_embedding = self._encode_query(query_text)
            
            # ChromaDB ê²€ìƒ‰ ì‹¤í–‰
            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=n_results,
                where=where,
                where_document=where_document,
                include=["documents", "metadatas", "distances"]  # ë°˜í™˜í•  í•„ë“œ ì§€ì •
            )
            
            # ê²€ìƒ‰ ì™„ë£Œ ë¡œê·¸
            logger.info(f"âœ… ê²€ìƒ‰ ì™„ë£Œ: {len(results['ids'][0])}ê°œ ê²°ê³¼")
            
            # ìƒìœ„ 3ê°œ ê²°ê³¼ì˜ ê±°ë¦¬ ì¶œë ¥ (í’ˆì§ˆ í™•ì¸ìš©)
            if results['distances'][0]:
                top_distances = results['distances'][0][:3]
                logger.info(f"   ìƒìœ„ 3ê°œ ê±°ë¦¬: {[f'{d:.4f}' for d in top_distances]}")
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            logger.error(f"   ì¿¼ë¦¬: '{query_text}'")
            logger.error(f"   í•„í„°: {where}")
            raise
    
    def _encode_query(self, query_text: str) -> List[float]:
        """
        ì¿¼ë¦¬ í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜
        
        í™˜ê²½ì— ë”°ë¼ ë‹¤ë¥¸ ë°©ì‹ ì‚¬ìš©:
        - GPU í™˜ê²½: ì‹¤ì œ sentence-transformers ëª¨ë¸
        - CPU í™˜ê²½: Mock ì„ë² ë”© (ê°œë°œìš©)
        
        Args:
            query_text (str): ì„ë² ë”©í•  í…ìŠ¤íŠ¸
            
        Returns:
            List[float]: ì„ë² ë”© ë²¡í„° (4096ì°¨ì›)
            
        Raises:
            Exception: ì„ë² ë”© ìƒì„± ì‹¤íŒ¨ ì‹œ
        """
        try:
            if self._is_gpu_environment and self._embedding_model is not None:
                # GPU í™˜ê²½: ì‹¤ì œ ëª¨ë¸ ì‚¬ìš©
                return self._encode_with_real_model(query_text)
            else:
                # CPU í™˜ê²½: Mock ì‚¬ìš©
                return self._encode_with_mock(query_text)
                
        except Exception as e:
            logger.error(f"âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            logger.warning("Mock ì„ë² ë”©ìœ¼ë¡œ ëŒ€ì²´")
            return self._encode_with_mock(query_text)
    
    def _encode_with_real_model(self, query_text: str) -> List[float]:
        """
        ì‹¤ì œ ëª¨ë¸ë¡œ ì„ë² ë”© ìƒì„± (ì½”ë©/RunPodìš©)
        
        sentence-transformersë¥¼ ì‚¬ìš©í•˜ì—¬ ê³ í’ˆì§ˆ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤.
        íŒ€ì›ì´ ì—…ë¡œë“œí•  ë•Œ ì‚¬ìš©í•œ ê²ƒê³¼ ë™ì¼í•œ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
        
        Args:
            query_text (str): ì„ë² ë”©í•  í…ìŠ¤íŠ¸
            
        Returns:
            List[float]: ì‹¤ì œ ì„ë² ë”© ë²¡í„°
        """
        try:
            # ë°°ì¹˜ í˜•íƒœë¡œ ì¸ì½”ë”© (ë‹¨ì¼ í…ìŠ¤íŠ¸ë„ ë¦¬ìŠ¤íŠ¸ë¡œ)
            embeddings = self._embedding_model.encode([query_text])
            
            # ì²« ë²ˆì§¸ (ìœ ì¼í•œ) ê²°ê³¼ ë°˜í™˜
            embedding_vector = embeddings[0].tolist()
            
            logger.debug(f"âœ… ì‹¤ì œ ëª¨ë¸ ì„ë² ë”© ìƒì„±: {len(embedding_vector)}ì°¨ì›")
            return embedding_vector
            
        except Exception as e:
            logger.error(f"ì‹¤ì œ ëª¨ë¸ ì„ë² ë”© ì‹¤íŒ¨: {e}")
            raise
    
    def _encode_with_mock(self, query_text: str) -> List[float]:
        
        """
        Mock ì„ë² ë”© ìƒì„± (ë¡œì»¬ ê°œë°œìš©)
        
        í…ìŠ¤íŠ¸ë¥¼ í•´ì‹œ ê¸°ë°˜ìœ¼ë¡œ ì¼ê´€ëœ ê°€ì§œ ë²¡í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        ê°™ì€ í…ìŠ¤íŠ¸ëŠ” í•­ìƒ ê°™ì€ ë²¡í„°ë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ ê°œë°œ/í…ŒìŠ¤íŠ¸ì— ìœ ìš©í•©ë‹ˆë‹¤.
        
        Args:
            query_text (str): ì„ë² ë”©í•  í…ìŠ¤íŠ¸
            
        Returns:
            List[float]: Mock ì„ë² ë”© ë²¡í„° (3584ì°¨ì›)
        """
    
        try:
            hash_obj = hashlib.md5(query_text.encode('utf-8'))
            seed = int(hash_obj.hexdigest(), 16) % (2**32)
            
            np.random.seed(seed)
            fake_embedding = np.random.normal(0, 1, 3584)  # 4096 â†’ 3584ë¡œ ë³€ê²½!
            
            norm = np.linalg.norm(fake_embedding)
            if norm > 0:
                fake_embedding = fake_embedding / norm
            
            logger.debug(f"ğŸ”„ Mock ì„ë² ë”© ìƒì„±: '{query_text[:30]}...' -> 3584ì°¨ì›")
            return fake_embedding.tolist()
            
        except Exception as e:
            logger.error(f"Mock ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return [0.0] * 3584  # 4096 â†’ 3584ë¡œ ë³€ê²½!
    
    def get_collection_info(self) -> Dict[str, Any]:
        """
        ì»¬ë ‰ì…˜ ì •ë³´ ë°˜í™˜
        
        í˜„ì¬ ì—°ê²°ëœ ì»¬ë ‰ì…˜ì˜ ë©”íƒ€ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        ë””ë²„ê¹…ì´ë‚˜ ìƒíƒœ í™•ì¸ì— ìœ ìš©í•©ë‹ˆë‹¤.
        
        Returns:
            Dict[str, Any]: ì»¬ë ‰ì…˜ ì •ë³´
                {
                    'name': ì»¬ë ‰ì…˜ ì´ë¦„,
                    'count': ì €ì¥ëœ ë¬¸ì„œ ê°œìˆ˜,
                    'metadata': ì»¬ë ‰ì…˜ ë©”íƒ€ë°ì´í„°,
                    'environment': í˜„ì¬ ì„ë² ë”© í™˜ê²½,
                    'model_loaded': ëª¨ë¸ ë¡œë“œ ìƒíƒœ
                }
        
        Examples:
            >>> client = get_vector_client()
            >>> info = client.get_collection_info()
            >>> print(f"ì»¬ë ‰ì…˜: {info['name']}, ë¬¸ì„œ ìˆ˜: {info['count']}")
        """
        try:
            return {
                "name": self.collection.name,
                "count": self.collection.count(),
                "metadata": self.collection.metadata,
                "environment": "GPU" if self._is_gpu_environment else "CPU (Mock)",
                "model_loaded": self._embedding_model is not None,
                "embedding_model": self.settings.EMBEDDING_MODEL
            }
        except Exception as e:
            logger.error(f"ì»¬ë ‰ì…˜ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}


# ============================================================
# ì‹±ê¸€í†¤ íŒ¨í„´
# ============================================================
# VectorClientëŠ” DB ì—°ê²°ê³¼ ëª¨ë¸ì„ ìœ ì§€í•˜ë¯€ë¡œ ë§¤ë²ˆ ìƒì„±í•˜ë©´ ë¹„íš¨ìœ¨ì ì…ë‹ˆë‹¤.
# ì „ì—­ ë³€ìˆ˜ë¡œ í•˜ë‚˜ì˜ ì¸ìŠ¤í„´ìŠ¤ë§Œ ìƒì„±í•˜ì—¬ ì¬ì‚¬ìš©í•©ë‹ˆë‹¤.
# ============================================================

_vector_client_instance = None  # ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ì €ì¥ ë³€ìˆ˜


def get_vector_client() -> VectorClient:
    """
    VectorClient ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜
    
    ì• í”Œë¦¬ì¼€ì´ì…˜ ì „ì²´ì—ì„œ í•˜ë‚˜ì˜ VectorClient ì¸ìŠ¤í„´ìŠ¤ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    ì²˜ìŒ í˜¸ì¶œ ì‹œ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•˜ê³ , ì´í›„ í˜¸ì¶œ ì‹œ ê°™ì€ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    ì‹±ê¸€í†¤ íŒ¨í„´ì„ ì‚¬ìš©í•˜ëŠ” ì´ìœ :
    - DB ì—°ê²°ì€ ë¹„ìš©ì´ í° ì‘ì—… (ë§¤ë²ˆ ìƒì„±í•˜ë©´ ëŠë¦¼)
    - ì„ë² ë”© ëª¨ë¸ ë¡œë”©ì€ ë©”ëª¨ë¦¬ ì§‘ì•½ì  (í•œ ë²ˆë§Œ ë¡œë“œ)
    - í•˜ë‚˜ì˜ ì—°ê²°ì„ ì¬ì‚¬ìš©í•˜ì—¬ ì„±ëŠ¥ í–¥ìƒ
    - ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±
    
    Returns:
        VectorClient: ì „ì—­ VectorClient ì¸ìŠ¤í„´ìŠ¤
    
    Examples:
        >>> # ì²« ë²ˆì§¸ í˜¸ì¶œ: ìƒˆ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ëª¨ë¸ ë¡œë”© í¬í•¨)
        >>> client1 = get_vector_client()
        >>> 
        >>> # ë‘ ë²ˆì§¸ í˜¸ì¶œ: ê°™ì€ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥)
        >>> client2 = get_vector_client()
        >>> 
        >>> # ë‘ ë³€ìˆ˜ëŠ” ê°™ì€ ê°ì²´ë¥¼ ê°€ë¦¬í‚´
        >>> assert client1 is client2  # True
    """
    global _vector_client_instance
    
    # ì¸ìŠ¤í„´ìŠ¤ê°€ ì—†ìœ¼ë©´ ìƒì„±
    if _vector_client_instance is None:
        logger.info("ğŸ”§ VectorClient ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì¤‘...")
        _vector_client_instance = VectorClient()
        
        # í™˜ê²½ ì •ë³´ ë¡œê·¸
        info = _vector_client_instance.get_collection_info()
        logger.info(f"ğŸ“Š í™˜ê²½: {info.get('environment')}")
        logger.info(f"ğŸ“Š ëª¨ë¸: {info.get('embedding_model')}")
        logger.info(f"ğŸ“Š ì»¬ë ‰ì…˜: {info.get('name')} ({info.get('count')}ê°œ ë¬¸ì„œ)")
    
    return _vector_client_instance


# ============================================================
# ê°œë°œ/í…ŒìŠ¤íŠ¸ìš© ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# ============================================================

def reset_vector_client():
    """
    VectorClient ì¸ìŠ¤í„´ìŠ¤ ì´ˆê¸°í™” (í…ŒìŠ¤íŠ¸ìš©)
    
    ì£¼ë¡œ í…ŒìŠ¤íŠ¸ë‚˜ ê°œë°œ ì¤‘ì— ì„¤ì •ì„ ë³€ê²½í•œ í›„ 
    ìƒˆë¡œìš´ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•˜ê³  ì‹¶ì„ ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    global _vector_client_instance
    _vector_client_instance = None
    logger.info("ğŸ”„ VectorClient ì¸ìŠ¤í„´ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤")


def test_vector_search(query: str = "í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬", n_results: int = 3) -> Dict[str, Any]:
    try:
        client = get_vector_client()
        results = client.search(query, n_results)
        
        print(f"ğŸ” í…ŒìŠ¤íŠ¸ ê²€ìƒ‰: '{query}'")
        print(f"ğŸ“Š ê²°ê³¼: {len(results['documents'][0])}ê°œ")
        
        # ìƒìœ„ ê²°ê³¼ ì¶œë ¥ (ìˆ˜ì •ëœ í‚¤ ì´ë¦„ ì‚¬ìš©)
        for i, (doc, meta, dist) in enumerate(zip(
            results['documents'][0][:3],
            results['metadatas'][0][:3], 
            results['distances'][0][:3]
        )):
            facility_name = meta.get('facility_name', 'N/A')
            region = f"{meta.get('region_city', '')}/{meta.get('region_gu', '')}"
            category = f"{meta.get('category1', '')}-{meta.get('category2', '')}"
            
            print(f"  {i+1}. {facility_name}")
            print(f"     ìœ„ì¹˜: {region}")
            print(f"     ë¶„ë¥˜: {category}")  
            print(f"     ê±°ë¦¬: {dist:.4f}")
        
        return results
        
    except Exception as e:
        logger.error(f"í…ŒìŠ¤íŠ¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return {"error": str(e)}
if __name__ == "__main__":
    # ì§ì ‘ ì‹¤í–‰ ì‹œ í…ŒìŠ¤íŠ¸
    print("ğŸ§ª VectorClient í…ŒìŠ¤íŠ¸ ì‹œì‘")
    test_vector_search("ë†€ì´í„°", 3)