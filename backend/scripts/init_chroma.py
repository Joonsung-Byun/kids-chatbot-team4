"""
ChromaDB ë°ì´í„° ì´ˆê¸°í™” ìŠ¤í¬ë¦½íŠ¸

CSV íŒŒì¼ì„ ì½ì–´ ChromaDBì— ë²¡í„° ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
ë¡œì»¬ ê°œë°œ ë° Docker í™˜ê²½ì—ì„œ ëª¨ë‘ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.
"""

import sys
import os
import pandas as pd
from pathlib import Path

# backend ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent.parent))

from utils.vector_client import get_vector_client
from utils.logger import logger


def load_csv_to_chroma(csv_path: str, batch_size: int = 100):
    """
    CSV íŒŒì¼ì„ ChromaDBì— ë¡œë“œ
    
    Args:
        csv_path: CSV íŒŒì¼ ê²½ë¡œ
        batch_size: ë°°ì¹˜ í¬ê¸° (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
    """
    try:
        # CSV ë¡œë“œ
        logger.info(f"ğŸ“‚ CSV íŒŒì¼ ë¡œë”©: {csv_path}")
        df = pd.read_csv(csv_path)
        logger.info(f"âœ… {len(df)}ê°œ í–‰ ë¡œë“œ ì™„ë£Œ")
        
        # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
        required_cols = ["facility_name", "description"]
        missing_cols = [col for col in required_cols if col not in df.columns]
        
        if missing_cols:
            raise ValueError(f"í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing_cols}")
        
        # NaN ì œê±°
        df = df.dropna(subset=required_cols)
        logger.info(f"ğŸ§¹ ì •ì œ í›„: {len(df)}ê°œ í–‰")
        
        # VectorClient ì´ˆê¸°í™”
        logger.info("ğŸ”— ChromaDB ì—°ê²° ì¤‘...")
        client = get_vector_client()
        
        # ë°°ì¹˜ ì²˜ë¦¬
        total_batches = (len(df) + batch_size - 1) // batch_size
        logger.info(f"ğŸ“¦ {total_batches}ê°œ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì—…ë¡œë“œ")
        
        for i in range(0, len(df), batch_size):
            batch_df = df.iloc[i:i+batch_size]
            batch_num = (i // batch_size) + 1
            
            logger.info(f"â³ ë°°ì¹˜ {batch_num}/{total_batches} ì²˜ë¦¬ ì¤‘...")
            
            # ë¬¸ì„œ ë° ë©”íƒ€ë°ì´í„° ì¤€ë¹„
            documents = batch_df["description"].tolist()
            metadatas = batch_df.to_dict("records")
            ids = [f"facility_{idx}" for idx in batch_df.index]
            
            # ChromaDBì— ì¶”ê°€
            client.add_documents(
                documents=documents,
                metadatas=metadatas,
                ids=ids
            )
            
            logger.info(f"âœ… ë°°ì¹˜ {batch_num} ì™„ë£Œ ({len(documents)}ê°œ ë¬¸ì„œ)")
        
        # ìµœì¢… í™•ì¸
        info = client.get_collection_info()
        logger.info(f"ğŸ‰ ë°ì´í„° ë¡œë“œ ì™„ë£Œ!")
        logger.info(f"ğŸ“Š ì´ ë¬¸ì„œ ìˆ˜: {info['count']}")
        logger.info(f"ğŸ“Š ì»¬ë ‰ì…˜: {info['name']}")
        
        return True
    
    except FileNotFoundError:
        logger.error(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}")
        return False
    
    except Exception as e:
        logger.error(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False


def verify_data():
    """ë°ì´í„° ë¡œë“œ ê²€ì¦"""
    try:
        logger.info("ğŸ” ë°ì´í„° ê²€ì¦ ì¤‘...")
        
        client = get_vector_client()
        info = client.get_collection_info()
        
        logger.info(f"âœ… ì»¬ë ‰ì…˜: {info['name']}")
        logger.info(f"âœ… ë¬¸ì„œ ìˆ˜: {info['count']}")
        logger.info(f"âœ… í™˜ê²½: {info['environment']}")
        
        # ìƒ˜í”Œ ê²€ìƒ‰
        results = client.search("ë†€ì´í„°", n_results=3)
        
        logger.info(f"ğŸ” ìƒ˜í”Œ ê²€ìƒ‰ ê²°ê³¼: {len(results['ids'][0])}ê°œ")
        
        for i, (doc, meta) in enumerate(zip(
            results['documents'][0][:3],
            results['metadatas'][0][:3]
        ), 1):
            logger.info(f"  {i}. {meta.get('facility_name', 'N/A')}")
        
        return True
    
    except Exception as e:
        logger.error(f"âŒ ê²€ì¦ ì‹¤íŒ¨: {e}")
        return False


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="CSV ë°ì´í„°ë¥¼ ChromaDBì— ë¡œë“œ"
    )
    parser.add_argument(
        "csv_path",
        type=str,
        help="CSV íŒŒì¼ ê²½ë¡œ"
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=100,
        help="ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 100)"
    )
    parser.add_argument(
        "--verify",
        action="store_true",
        help="ë¡œë“œ í›„ ê²€ì¦"
    )
    
    args = parser.parse_args()
    
    # ë°ì´í„° ë¡œë“œ
    success = load_csv_to_chroma(args.csv_path, args.batch_size)
    
    if not success:
        sys.exit(1)
    
    # ê²€ì¦
    if args.verify:
        if not verify_data():
            sys.exit(1)
    
    logger.info("ğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")


if __name__ == "__main__":
    main()